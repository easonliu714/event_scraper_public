# -*- coding: utf-8 -*-
import asyncio
import aiohttp
import random
import json
import re
import time
import logging
import os
from datetime import datetime
from urllib.parse import urljoin
from pathlib import Path
from bs4 import BeautifulSoup

# =========================
# ğŸ› ï¸ è¨­å®šå€ & æ—¥èªŒé…ç½®
# =========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

OUTPUT_DIR = Path("docs")
OUTPUT_FILE = OUTPUT_DIR / "data.json"

# LINE Notify Token (å¾ GitHub Secrets è®€å–)
LINE_TOKEN = os.environ.get("LINE_TOKEN")

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
]

# è©³ç´°é ç¶²å€ç™½åå–®
DETAIL_URL_WHITELIST = {
    "æ‹“å…ƒå”®ç¥¨": re.compile(r"^https?://(www\.)?tixcraft\.com/activity/detail/[A-Za-z0-9_-]+", re.I),
    "KKTIX": re.compile(r"^https?://[a-z0-9-]+\.kktix\.cc/events/[A-Za-z0-9-_]+", re.I),
    "OPENTIX": re.compile(r"^https?://(www\.)?opentix\.life/event/\d+", re.I),
    "å¹´ä»£å”®ç¥¨": re.compile(r"^https?://(www\.)?ticket\.com\.tw/application/UTK02/UTK0201_\.aspx\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "UDNå”®ç¥¨ç¶²": re.compile(r"^https?://(www\.)?tickets\.udnfunlife\.com/application/UTK02/UTK0201_\.aspx\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "TixFunå”®ç¥¨ç¶²": re.compile(r"^https?://(www\.)?tixfun\.com/UTK0201_\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "å¯¬å®": re.compile(r"^https?://(www\.)?kham\.com\.tw/application/UTK02/UTK0201_\.aspx\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "Event Go": re.compile(r"^https?://eventgo\.bnextmedia\.com\.tw/event/detail[^\s]*$", re.I),
    "iNDIEVOX": re.compile(r"^https?://(www\.)?indievox\.com/activity/detail/[0-9_]+", re.I),
    "ibon": re.compile(r"^https?://ticket\.ibon\.com\.tw/ActivityDetail/.*", re.I),
    "è¯å±±1914": re.compile(r"^https?://(www\.)?huashan1914\.com/w/huashan1914/exhibition.*", re.I),
    "æ¾å±±æ–‡å‰µ": re.compile(r"^https?://(www\.)?songshanculturalpark\.org/exhibition.*", re.I),
    "KidsClub": re.compile(r"^https?://(www\.)?kidsclub\.com\.tw/.*", re.I),
    "StrollTimes": re.compile(r"^https?://strolltimes\.com/.*", re.I),
    "å°åŒ—ä¸–è²¿": re.compile(r"^https?://(www\.)?twtc\.com\.tw/.*", re.I),
    "ä¸­æ­£ç´€å¿µå ‚": re.compile(r"^https?://(www\.)?cksmh\.gov\.tw/activitybee_.*", re.I),
    "Klook": re.compile(r"^https?://(www\.)?klook\.com/.*", re.I), # ä¿ç•™æ“´å……æ€§
}

# =========================
# ğŸ§© è¼”åŠ©å·¥å…·å‡½å¼
# =========================
def get_headers(referer=None):
    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
    }
    if referer: headers['Referer'] = referer
    return headers

def safe_get_text(element, default="è©³å…§æ–‡"):
    if element and hasattr(element, 'get_text'):
        text = element.get_text(strip=True)
        return text if text else default
    return default

def get_event_category_from_title(title):
    if not title: return "å…¶ä»–"
    title_lower = title.lower()
    category_mapping = {
        "éŸ³æ¨‚æœƒ/æ¼”å”±æœƒ": ["éŸ³æ¨‚æœƒ", "æ¼”å”±æœƒ", "ç¨å¥æœƒ", "åˆå”±", "äº¤éŸ¿", "ç®¡æ¨‚", "åœ‹æ¨‚", "å¼¦æ¨‚", "é‹¼ç´", "æç´", "å·¡æ¼”", "fan concert", "fancon", "éŸ³æ¨‚ç¯€", "çˆµå£«", "æ¼”å¥", "æ­Œæ‰‹", "æ¨‚åœ˜", "tour", "live", "concert", "solo", "recital", "é›»éŸ³æ´¾å°", "è—äººè¦‹é¢æœƒ"],
        "éŸ³æ¨‚åŠ‡/æ­ŒåŠ‡": ["éŸ³æ¨‚åŠ‡", "æ­ŒåŠ‡", "musical", "opera"],
        "æˆ²åŠ‡è¡¨æ¼”": ["æˆ²åŠ‡", "èˆå°åŠ‡", "åŠ‡åœ˜", "åŠ‡å ´", "å–œåŠ‡", "å…¬æ¼”", "æŒä¸­æˆ²", "æ­Œä»”æˆ²", "è±«åŠ‡", "è©±åŠ‡", "ç›¸è²", "å¸ƒè¢‹æˆ²", "äº¬åŠ‡", "å´‘åŠ‡", "è—æ–‡æ´»å‹•"],
        "èˆè¹ˆè¡¨æ¼”": ["èˆè¹ˆ", "èˆä½œ", "èˆåœ˜", "èŠ­è•¾", "èˆåŠ‡", "ç¾ä»£èˆ", "æ°‘æ—èˆ", "è¸¢è¸èˆ", "zumba"],
        "å±•è¦½/åšè¦½": ["å±•è¦½", "ç‰¹å±•", "åšç‰©é¤¨", "ç¾è¡“é¤¨", "è—è¡“å±•", "ç•«å±•", "æ”å½±å±•", "æ–‡ç‰©å±•", "ç§‘å­¸å±•", "åšè¦½æœƒ", "å‹•æ¼«", "å±•å‡º"],
        "è¦ªå­æ´»å‹•": ["è¦ªå­", "å…’ç«¥", "å¯¶å¯¶", "å®¶åº­", "å°æœ‹å‹", "ç«¥è©±", "å¡é€š", "å‹•ç•«", "é«”é©—"],
        "é›»å½±æ”¾æ˜ ": ["é›»å½±", "å½±å±•", "æ•¸ä½ä¿®å¾©", "æ”¾æ˜ ", "é¦–æ˜ ", "ç´€éŒ„ç‰‡", "å‹•ç•«é›»å½±"],
        "é«”è‚²è³½äº‹": ["æ£’çƒ", "ç±ƒçƒ", "éŒ¦æ¨™è³½", "é‹å‹•æœƒ", "è¶³çƒ", "ç¾½çƒ", "ç¶²çƒ", "é¦¬æ‹‰æ¾", "è·¯è·‘", "æ¸¸æ³³", "é«”æ“", "championship", "éŠæˆ²ç«¶è³½"],
        "è¬›åº§/å·¥ä½œåŠ": ["å·¥ä½œåŠ", "èª²ç¨‹", "å°è®€", "æ²™é¾", "è¬›åº§", "é«”é©—", "ç ”ç¿’", "åŸ¹è¨“", "è«–å£‡", "ç ”è¨æœƒ", "åº§è«‡", "workshop", "è·å ´å·¥ä½œè¡“", "è³‡è¨Šç§‘æŠ€", "AI", "Python", "ç«¶è³½", "å‰µä½œ", "çºç¹"],
        "å¨›æ¨‚è¡¨æ¼”": ["è„«å£ç§€", "é­”è¡“", "é›œæŠ€", "é¦¬æˆ²", "ç‰¹æŠ€", "é­”å¹»", "ç¶œè—", "å¨›æ¨‚", "ç§€å ´", "è¡¨æ¼”ç§€", "ç¤¾ç¾¤æ´»å‹•", "æ´¾å°", "å¸‚é›†"],
        "å…¶ä»–": ["æ—…éŠ", "ç¾é£Ÿ", "å…¬ç›Š"]
    }
    for category, keywords in category_mapping.items():
        if any(keyword in title_lower for keyword in keywords):
            return category
    return "å…¶ä»–"

async def fetch_text(session, url, headers=None, timeout_sec=15):
    if not headers: headers = get_headers()
    try:
        start_time = time.time()
        async with session.get(url, headers=headers, ssl=False, timeout=timeout_sec) as resp:
            duration = time.time() - start_time
            if resp.status != 200:
                logger.warning(f"âŒ HTTP {resp.status} - {url}")
                return None
            text = await resp.text()
            return text
    except asyncio.TimeoutError:
        logger.error(f"â³ è«‹æ±‚é€¾æ™‚: {url}")
        return None
    except Exception as e:
        logger.error(f"ğŸ’¥ è«‹æ±‚ç•°å¸¸: {url} - {e}")
        return None

def filter_links_for_platform(links, base_url, platform_name):
    events = []
    seen_urls = set()
    wl = DETAIL_URL_WHITELIST.get(platform_name)

    for link in links:
        href = link.get('href', '')
        if not href: continue
        full_url = urljoin(base_url, href).split('#')[0]

        if full_url in seen_urls: continue
        if wl and not wl.match(full_url): continue

        # --- å¼·åŒ–ç‰ˆæ¨™é¡Œè§£æé‚è¼¯ ---
        title = None
        
        # 1. å„ªå…ˆå¾ title å±¬æ€§ç²å–
        title = link.get('title')
        
        # 2. ç²å–å…§éƒ¨åœ–ç‰‡çš„ alt
        if not title or title.strip() in ['è©³å…§æ–‡', 'è©³ç´°è³‡è¨Š', 'è³¼ç¥¨']:
            img = link.find('img')
            if img: title = img.get('alt') or img.get('title')
            
        # 3. ç²å–æ‰€æœ‰å…§éƒ¨æ–‡å­—ä¸¦æ¸…ç†
        if not title or title.strip() in ['è©³å…§æ–‡', 'è©³ç´°è³‡è¨Š', 'è³¼ç¥¨']:
            # åˆä½µå…§éƒ¨æ‰€æœ‰ span, div çš„æ–‡å­—ï¼Œä¸¦éæ¿¾æ‰ç´”æ•¸å­—(ç¥¨åƒ¹)æˆ–çŸ­è©
            title = link.get_text(" ", strip=True)
            
        # 4. çµ‚æ¥µæ¸…ç†ï¼šç§»é™¤é›œè¨Š
        if title:
            # ç§»é™¤å¸¸è¦‹çš„è¼”åŠ©æ–‡å­—
            noise = ['ç«‹å³è³¼ç¥¨', 'è©³ç´°å…§å®¹', 'Read More', 'æ´»å‹•è©³æƒ…', 'æŸ¥çœ‹æ›´å¤š', 'å·²çµæŸ']
            for n in noise:
                title = title.replace(n, "")
            # ç§»é™¤æ—¥æœŸæ ¼å¼ (ä¾‹å¦‚ 2026/01/01)
            title = re.sub(r'\d{4}[-/]\d{1,2}[-/]\d{1,2}', '', title)
            title = title.strip()

        # å¦‚æœé‚„æ˜¯æŠ“ä¸åˆ°ï¼Œæˆ–è€…æŠ“åˆ°å¤ªçŸ­çš„æ±è¥¿ï¼Œå‰‡æ”¾æ£„è©²é€£çµ
        if not title or len(title) < 4:
            continue

        # åœ–ç‰‡æŠ“å–
        img_url = None
        img_tag = link.find('img')
        if img_tag: img_url = img_tag.get('src')
        if img_url and not img_url.startswith('http'):
            img_url = urljoin(base_url, img_url)

        events.append({
            'title': title,
            'url': full_url,
            'platform': platform_name,
            'img_url': img_url,
            'date': "è©³å…§æ–‡",
            'type': get_event_category_from_title(title),
            'scraped_at': datetime.now().isoformat()
        })
        seen_urls.add(full_url)

    logger.info(f"[{platform_name}] å¼·åŒ–è§£æå®Œæˆ: æ‰¾åˆ° {len(events)} ç­†")
    return events

# =========================
# ğŸ•·ï¸ å„å¹³å°çˆ¬èŸ²å‡½å¼ (19å¹³å°å…¨æ”¶éŒ„)
# =========================

async def fetch_kktix_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• KKTIX çˆ¬èŸ²...")
    base_url = "https://kktix.com/events"
    categories = [f"{base_url}?category_id={i}" for i in [2, 6, 4, 3, 8]] + ["https://kktix.com/"]
    all_events = []
    seen = set()
    for url in categories:
        await asyncio.sleep(random.uniform(1, 2))
        html = await fetch_text(session, url, headers=get_headers('https://kktix.com/'))
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="/events/"], .event-item a, .event-card a')
        events = filter_links_for_platform(links, "https://kktix.com/", "KKTIX")
        for e in events:
            if e['url'] not in seen:
                all_events.append(e)
                seen.add(e['url'])
    return all_events

async def fetch_accupass_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• ACCUPASS çˆ¬èŸ²...")
    base_url = "https://www.accupass.com/search"
    target_urls = [f"{base_url}?q={k}" for k in ["éŸ³æ¨‚", "è—æ–‡", "å­¸ç¿’", "ç§‘æŠ€"]] + ["https://www.accupass.com/?area=north"]
    all_events = []
    seen = set()
    for url in target_urls:
        await asyncio.sleep(random.uniform(2, 4))
        html = await fetch_text(session, url, headers=get_headers('https://www.accupass.com/'))
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        candidates = soup.find_all('a', href=re.compile(r'^/event/([A-Za-z0-9]+)'))
        for link in candidates:
            href = link.get('href')
            if not href or 'javascript' in href: continue
            full_url = urljoin("https://www.accupass.com", href).split('?')[0]
            if full_url in seen: continue
            title = safe_get_text(link.find('h3')) or safe_get_text(link)
            if len(title) < 2: continue
            img_tag = link.find('img')
            img_url = img_tag.get('src') if img_tag else None
            all_events.append({
                "title": title, "url": full_url, "platform": "ACCUPASS", "date": "è©³å…§æ–‡", "img_url": img_url,
                "type": get_event_category_from_title(title), "scraped_at": datetime.now().isoformat()
            })
            seen.add(full_url)
    return all_events

async def fetch_tixcraft_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• æ‹“å…ƒå”®ç¥¨ çˆ¬èŸ²...")
    urls = ["https://tixcraft.com/activity", "https://tixcraft.com/activity/list/select_type/all"]
    all_events = []
    seen = set()
    for url in urls:
        await asyncio.sleep(1.5)
        html = await fetch_text(session, url, headers=get_headers('https://tixcraft.com/'))
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="/activity/detail/"]')
        events = filter_links_for_platform(links, "https://tixcraft.com/", "æ‹“å…ƒå”®ç¥¨")
        for e in events:
            if e['url'] not in seen: all_events.append(e); seen.add(e['url'])
    return all_events

async def fetch_kham_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• å¯¬å®å”®ç¥¨ çˆ¬èŸ²...")
    cats = {"205": "éŸ³æ¨‚æœƒ", "231": "å±•è¦½", "116": "æˆ²åŠ‡", "129": "è¦ªå­"}
    all_events = []
    seen = set()
    for cat_id, cat_name in cats.items():
        url = f"https://kham.com.tw/application/UTK01/UTK0101_06.aspx?TYPE=1&CATEGORY={cat_id}"
        await asyncio.sleep(1)
        html = await fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="UTK0201"]')
        events = filter_links_for_platform(links, "https://kham.com.tw/", "å¯¬å®")
        for e in events:
            if e['url'] not in seen:
                e['type'] = cat_name
                all_events.append(e); seen.add(e['url'])
    return all_events

async def fetch_opentix_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• OPENTIX çˆ¬èŸ²...")
    html = await fetch_text(session, "https://www.opentix.life/event")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="/event/"]')
    return filter_links_for_platform(links, "https://www.opentix.life/", "OPENTIX")

async def fetch_udn_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• UDNå”®ç¥¨ çˆ¬èŸ²...")
    urls = ["https://tickets.udnfunlife.com/application/UTK01/UTK0101_03.aspx?Category=77&kdid=cateList", 
            "https://tickets.udnfunlife.com/application/UTK01/UTK0101_03.aspx?Category=231&kdid=cateList"]
    all_events = []
    seen = set()
    for url in urls:
        await asyncio.sleep(1)
        html = await fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="UTK0201"]')
        events = filter_links_for_platform(links, "https://tickets.udnfunlife.com/", "UDNå”®ç¥¨ç¶²")
        for e in events:
            if e['url'] not in seen: all_events.append(e); seen.add(e['url'])
    return all_events

async def fetch_famiticket_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• FamiTicket çˆ¬èŸ²...")
    html = await fetch_text(session, "https://www.famiticket.com.tw/Home")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select("a[href*='Content/Home/Activity']")
    return filter_links_for_platform(links, "https://www.famiticket.com.tw", "FamiTicket")

async def fetch_era_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• å¹´ä»£å”®ç¥¨ çˆ¬èŸ²...")
    html = await fetch_text(session, "https://ticket.com.tw/application/UTK01/UTK0101_06.aspx?TYPE=1&CATEGORY=77")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="UTK0201"]')
    return filter_links_for_platform(links, "https://ticket.com.tw", "å¹´ä»£å”®ç¥¨")

async def fetch_tixfun_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• TixFun çˆ¬èŸ²...")
    html = await fetch_text(session, "https://tixfun.com/UTK0101_?TYPE=1&CATEGORY=77")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="UTK0201"]')
    return filter_links_for_platform(links, "https://tixfun.com", "TixFunå”®ç¥¨ç¶²")

async def fetch_eventgo_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• Event Go çˆ¬èŸ²...")
    html = await fetch_text(session, "https://eventgo.bnextmedia.com.tw/")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="/event/detail"]')
    return filter_links_for_platform(links, "https://eventgo.bnextmedia.com.tw/", "Event Go")

async def fetch_beclass_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• BeClass çˆ¬èŸ²...")
    html = await fetch_text(session, "https://www.beclass.com/default.php?name=ShowList&op=recent")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select("a[href*='rid=']")
    return filter_links_for_platform(links, "https://www.beclass.com", "BeClass")

async def fetch_indievox_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• iNDIEVOX çˆ¬èŸ²...")
    html = await fetch_text(session, "https://www.indievox.com/activity/list")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="/activity/detail"]')
    return filter_links_for_platform(links, "https://www.indievox.com", "iNDIEVOX")

async def fetch_ibon_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• ibon çˆ¬èŸ²...")
    html = await fetch_text(session, "https://ticket.ibon.com.tw/Activity/Index")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="ActivityDetail"]')
    return filter_links_for_platform(links, "https://ticket.ibon.com.tw", "ibon")

async def fetch_huashan_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• è¯å±±1914 çˆ¬èŸ²...")
    html = await fetch_text(session, "https://www.huashan1914.com/w/huashan1914/exhibition")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('.card-body a')
    return filter_links_for_platform(links, "https://www.huashan1914.com", "è¯å±±1914")

async def fetch_songshan_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• æ¾å±±æ–‡å‰µ çˆ¬èŸ²...")
    html = await fetch_text(session, "https://www.songshanculturalpark.org/exhibition")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('.exhibition-list a')
    return filter_links_for_platform(links, "https://www.songshanculturalpark.org", "æ¾å±±æ–‡å‰µ")

async def fetch_stroll_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• StrollTimes çˆ¬èŸ²...")
    html = await fetch_text(session, "https://strolltimes.com/")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href]')
    return filter_links_for_platform(links, "https://strolltimes.com", "StrollTimes")

async def fetch_kidsclub_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• KidsClub çˆ¬èŸ²...")
    html = await fetch_text(session, "https://www.kidsclub.com.tw/")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href]')
    return filter_links_for_platform(links, "https://www.kidsclub.com.tw", "KidsClub")

async def fetch_wtc_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• å°åŒ—ä¸–è²¿ çˆ¬èŸ²...")
    html = await fetch_text(session, "https://www.twtc.com.tw/exhibition_list.aspx")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="exhibition_detail"]')
    return filter_links_for_platform(links, "https://www.twtc.com.tw", "å°åŒ—ä¸–è²¿")

async def fetch_cksmh_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• ä¸­æ­£ç´€å¿µå ‚ çˆ¬èŸ²...")
    # [ä¿®æ­£] V34 åŸå§‹ç¶²å€ 404ï¼Œæ”¹ç”¨æ–°ç¶²å€
    url = "https://www.cksmh.gov.tw/activitybee_list.aspx?n=105" 
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="activitybee_"]')
    return filter_links_for_platform(links, "https://www.cksmh.gov.tw", "ä¸­æ­£ç´€å¿µå ‚")

# =========================
# ğŸ’¾ è³‡æ–™è™•ç† & LINE é€šçŸ¥
# =========================
async def send_line_notify(message):
    if not LINE_TOKEN: return
    url = "https://notify-api.line.me/api/notify"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}"}
    data = {"message": message}
    async with aiohttp.ClientSession() as session:
        async with session.post(url, headers=headers, data=data) as resp:
            if resp.status == 200: logger.info("âœ… LINE é€šçŸ¥ç™¼é€æˆåŠŸ")
            else: logger.error(f"âŒ LINE é€šçŸ¥å¤±æ•—: {resp.status}")

def load_existing_data():
    if not OUTPUT_FILE.exists(): return []
    try:
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except: return []

async def save_data_and_notify(new_events):
    existing_events = load_existing_data()
    existing_map = {e['url']: e for e in existing_events}
    
    added_events = []
    
    for event in new_events:
        url = event['url']
        if url not in existing_map:
            existing_map[url] = event
            added_events.append(event)
        else:
            existing_map[url].update(event)
    
    # å­˜æª”
    final_list = list(existing_map.values())
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_list, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“Š ç¸½ç­†æ•¸: {len(final_list)} | ğŸ†• æ–°å¢: {len(added_events)}")

    # LINE é€šçŸ¥é‚è¼¯ (åƒ…é€šçŸ¥æ–°å¢çš„)
    if added_events and LINE_TOKEN:
        logger.info(f"ğŸ“¨ æº–å‚™ç™¼é€ LINE é€šçŸ¥ ({len(added_events)} ç­†)...")
        # ç‚ºäº†é¿å…æ´—ç‰ˆï¼Œåªå–å‰ 5 ç­† + æ‘˜è¦
        msg = f"\nğŸ”¥ ç™¼ç¾ {len(added_events)} å€‹æ–°æ´»å‹•ï¼\n"
        for e in added_events[:5]:
            msg += f"\nğŸ“Œ {e['title'][:20]}...\nğŸ”— {e['url']}\n"
        if len(added_events) > 5:
            msg += f"\n...é‚„æœ‰ {len(added_events)-5} ç­†ï¼Œè«‹ä¸Šç¶²é æŸ¥çœ‹ï¼"
        
        await send_line_notify(msg)

async def main():
    logger.info("ğŸ”¥ çˆ¬èŸ²ç¨‹å¼é–‹å§‹åŸ·è¡Œ (Web V34 Full + LINE)...")
    connector = aiohttp.TCPConnector(ssl=False)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [
            fetch_kktix_events_list(session), fetch_accupass_events_list(session),
            fetch_tixcraft_events_list(session), fetch_kham_events_list(session),
            fetch_opentix_events_list(session), fetch_udn_events_list(session),
            fetch_famiticket_events_list(session), fetch_era_events_list(session),
            fetch_tixfun_events_list(session), fetch_eventgo_events_list(session),
            fetch_beclass_events_list(session), fetch_indievox_events_list(session),
            fetch_ibon_events_list(session), fetch_huashan_events_list(session),
            fetch_songshan_events_list(session), fetch_stroll_events_list(session),
            fetch_kidsclub_events_list(session), fetch_wtc_events_list(session),
            fetch_cksmh_events_list(session)
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        all_new = []
        for res in results:
            if isinstance(res, list): all_new.extend(res)
            else: logger.error(f"âŒ ä»»å‹™å¤±æ•—: {res}")

        logger.info(f"ğŸ” å…±æŠ“å–åˆ° {len(all_new)} ç­†æœ‰æ•ˆè³‡æ–™")
        await save_data_and_notify(all_new)

if __name__ == "__main__":
    asyncio.run(main())
