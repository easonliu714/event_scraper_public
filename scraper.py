# -*- coding: utf-8 -*-
import asyncio
import aiohttp
import random
import json
import re
import time
import logging
import os
from datetime import datetime
from urllib.parse import urljoin
from pathlib import Path
from bs4 import BeautifulSoup

# =========================
# ğŸ› ï¸ è¨­å®šå€
# =========================
# åŒæ™‚è¼¸å‡ºåˆ° Console å’Œ File
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler("scraper.log", encoding='utf-8', mode='w'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

OUTPUT_DIR = Path("docs")
OUTPUT_FILE = OUTPUT_DIR / "data.json"
LINE_TOKEN = os.environ.get("LINE_TOKEN")

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
]

# è©³ç´°é ç¶²å€ç™½åå–® (V34)
DETAIL_URL_WHITELIST = {
    "æ‹“å…ƒå”®ç¥¨": re.compile(r"^https?://(www\.)?tixcraft\.com/activity/detail/[A-Za-z0-9_-]+", re.I),
    "KKTIX": re.compile(r"^https?://[a-z0-9-]+\.kktix\.cc/events/[A-Za-z0-9-_]+", re.I),
    "OPENTIX": re.compile(r"^https?://(www\.)?opentix\.life/event/\d+", re.I),
    "å¹´ä»£å”®ç¥¨": re.compile(r"^https?://(www\.)?ticket\.com\.tw/application/UTK02/UTK0201_\.aspx\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "UDNå”®ç¥¨ç¶²": re.compile(r"^https?://(www\.)?tickets\.udnfunlife\.com/application/UTK02/UTK0201_\.aspx\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "TixFunå”®ç¥¨ç¶²": re.compile(r"^https?://(www\.)?tixfun\.com/UTK0201_\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "å¯¬å®": re.compile(r"^https?://(www\.)?kham\.com\.tw/application/UTK02/UTK0201_\.aspx\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "Event Go": re.compile(r"^https?://eventgo\.bnextmedia\.com\.tw/event/detail[^\s]*$", re.I),
    "iNDIEVOX": re.compile(r"^https?://(www\.)?indievox\.com/activity/detail/[0-9_]+", re.I),
    "ibon": re.compile(r"^https?://ticket\.ibon\.com\.tw/ActivityDetail/.*", re.I),
    "è¯å±±1914": re.compile(r"^https?://(www\.)?huashan1914\.com/w/huashan1914/exhibition.*", re.I),
    "æ¾å±±æ–‡å‰µ": re.compile(r"^https?://(www\.)?songshanculturalpark\.org/exhibition.*", re.I),
    "KidsClub": re.compile(r"^https?://(www\.)?kidsclub\.com\.tw/.*", re.I),
    "StrollTimes": re.compile(r"^https?://strolltimes\.com/.*", re.I),
    "å°åŒ—ä¸–è²¿": re.compile(r"^https?://(www\.)?twtc\.com\.tw/.*", re.I),
    "ä¸­æ­£ç´€å¿µå ‚": re.compile(r"^https?://(www\.)?cksmh\.gov\.tw/activitybee_.*", re.I),
    "Klook": re.compile(r"^https?://(www\.)?klook\.com/.*", re.I),
}

# =========================
# ğŸ§© è¼”åŠ©å·¥å…·å‡½å¼
# =========================
def get_headers(referer=None):
    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
    }
    if referer: headers['Referer'] = referer
    return headers

def safe_get_text(element, default="è©³å…§æ–‡"):
    if element and hasattr(element, 'get_text'):
        text = element.get_text(strip=True)
        return text if text else default
    return default

def get_event_category_from_title(title):
    if not title: return "å…¶ä»–"
    title_lower = title.lower()
    category_mapping = {
        "éŸ³æ¨‚æœƒ/æ¼”å”±æœƒ": ["éŸ³æ¨‚æœƒ", "æ¼”å”±æœƒ", "ç¨å¥æœƒ", "åˆå”±", "äº¤éŸ¿", "ç®¡æ¨‚", "åœ‹æ¨‚", "å¼¦æ¨‚", "é‹¼ç´", "æç´", "å·¡æ¼”", "fan concert", "fancon", "éŸ³æ¨‚ç¯€", "çˆµå£«", "æ¼”å¥", "æ­Œæ‰‹", "æ¨‚åœ˜", "tour", "live", "concert", "solo", "recital", "é›»éŸ³æ´¾å°", "è—äººè¦‹é¢æœƒ", "éŸ³æ¨‚ç¥­"],
        "éŸ³æ¨‚åŠ‡/æ­ŒåŠ‡": ["éŸ³æ¨‚åŠ‡", "æ­ŒåŠ‡", "musical", "opera"],
        "æˆ²åŠ‡è¡¨æ¼”": ["æˆ²åŠ‡", "èˆå°åŠ‡", "åŠ‡åœ˜", "åŠ‡å ´", "å–œåŠ‡", "å…¬æ¼”", "æŒä¸­æˆ²", "æ­Œä»”æˆ²", "è±«åŠ‡", "è©±åŠ‡", "ç›¸è²", "å¸ƒè¢‹æˆ²", "äº¬åŠ‡", "å´‘åŠ‡", "è—æ–‡æ´»å‹•"],
        "èˆè¹ˆè¡¨æ¼”": ["èˆè¹ˆ", "èˆä½œ", "èˆåœ˜", "èŠ­è•¾", "èˆåŠ‡", "ç¾ä»£èˆ", "æ°‘æ—èˆ", "è¸¢è¸èˆ", "zumba"],
        "å±•è¦½/åšè¦½": ["å±•è¦½", "ç‰¹å±•", "åšç‰©é¤¨", "ç¾è¡“é¤¨", "è—è¡“å±•", "ç•«å±•", "æ”å½±å±•", "æ–‡ç‰©å±•", "ç§‘å­¸å±•", "åšè¦½æœƒ", "å‹•æ¼«", "å±•å‡º"],
        "è¦ªå­æ´»å‹•": ["è¦ªå­", "å…’ç«¥", "å¯¶å¯¶", "å®¶åº­", "å°æœ‹å‹", "ç«¥è©±", "å¡é€š", "å‹•ç•«", "é«”é©—", "ç‡ŸéšŠ", "å†¬ä»¤ç‡Ÿ", "å¤ä»¤ç‡Ÿ"],
        "é›»å½±æ”¾æ˜ ": ["é›»å½±", "å½±å±•", "æ•¸ä½ä¿®å¾©", "æ”¾æ˜ ", "é¦–æ˜ ", "ç´€éŒ„ç‰‡", "å‹•ç•«é›»å½±"],
        "é«”è‚²è³½äº‹": ["æ£’çƒ", "ç±ƒçƒ", "éŒ¦æ¨™è³½", "é‹å‹•æœƒ", "è¶³çƒ", "ç¾½çƒ", "ç¶²çƒ", "é¦¬æ‹‰æ¾", "è·¯è·‘", "æ¸¸æ³³", "é«”æ“", "championship", "éŠæˆ²ç«¶è³½"],
        "è¬›åº§/å·¥ä½œåŠ": ["å·¥ä½œåŠ", "èª²ç¨‹", "å°è®€", "æ²™é¾", "è¬›åº§", "é«”é©—", "ç ”ç¿’", "åŸ¹è¨“", "è«–å£‡", "ç ”è¨æœƒ", "åº§è«‡", "workshop", "è·å ´å·¥ä½œè¡“", "è³‡è¨Šç§‘æŠ€", "AI", "Python", "ç«¶è³½", "å‰µä½œ", "çºç¹"],
        "å¨›æ¨‚è¡¨æ¼”": ["è„«å£ç§€", "é­”è¡“", "é›œæŠ€", "é¦¬æˆ²", "ç‰¹æŠ€", "é­”å¹»", "ç¶œè—", "å¨›æ¨‚", "ç§€å ´", "è¡¨æ¼”ç§€", "ç¤¾ç¾¤æ´»å‹•", "æ´¾å°", "å¸‚é›†"],
        "å…¶ä»–": ["æ—…éŠ", "ç¾é£Ÿ", "å…¬ç›Š"]
    }
    for category, keywords in category_mapping.items():
        if any(keyword in title_lower for keyword in keywords):
            return category
    return "å…¶ä»–"

async def fetch_text(session, url, headers=None, timeout_sec=20):
    if not headers: headers = get_headers()
    try:
        async with session.get(url, headers=headers, ssl=False, timeout=timeout_sec) as resp:
            if resp.status != 200:
                logger.warning(f"âŒ HTTP {resp.status} - {url}")
                return None
            # è‡ªå‹•åµæ¸¬ç·¨ç¢¼ï¼Œè§£æ±ºä¸­æ–‡äº‚ç¢¼å°è‡´æ¨™é¡ŒæŠ“ä¸åˆ°çš„å•é¡Œ
            content_type = resp.headers.get('Content-Type', '').lower()
            if 'charset' in content_type:
                return await resp.text()
            else:
                # è‹¥ç„¡æŒ‡å®šï¼Œå˜—è©¦è®€å– bytes ä¸¦è‡ªå‹•è§£ç¢¼
                data = await resp.read()
                try:
                    return data.decode('utf-8')
                except:
                    try:
                        return data.decode('big5') # å˜—è©¦ Big5 (éƒ¨åˆ†å°ç£èˆŠç¶²ç«™)
                    except:
                        return data.decode('utf-8', errors='ignore')
    except Exception as e:
        logger.error(f"ğŸ’¥ è«‹æ±‚ç•°å¸¸: {url} - {e}")
        return None

# =========================
# â˜… æ ¸å¿ƒéæ¿¾é‚è¼¯ (å®Œå…¨ç§»æ¤ V34 ä¸¦å¢å¼·) â˜…
# =========================
def filter_links_for_platform(links, base_url, platform_name):
    events = []
    seen_urls = set()
    wl = DETAIL_URL_WHITELIST.get(platform_name)
    
    logger.info(f"[{platform_name}] åŸå§‹é€£çµæ•¸: {len(links)}")

    for link in links:
        href = link.get('href', '')
        if not href: continue
        full_url = urljoin(base_url, href).split('#')[0]

        # å¹³å°ç‰¹æ®Šæ’é™¤
        if platform_name == "Event Go" and not full_url.startswith("https://eventgo.bnextmedia.com.tw/event/detail"): continue
            
        if full_url in seen_urls: continue
        if wl and not wl.match(full_url): 
            # logger.debug(f"è·³ééç™½åå–®é€£çµ: {full_url}") # å¤ªå¤šé›œè¨Šå…ˆè¨»è§£
            continue

        # --- æ¨™é¡Œè§£æç­–ç•¥ (æ¨¡ä»¿ V34 + å¢å¼·) ---
        title = link.get('title')
        
        # V34 é‚è¼¯: å„ªå…ˆ title -> åœ–ç‰‡ alt -> æ–‡å­—
        if not title or title.strip() in ['è©³å…§æ–‡', 'è©³ç´°è³‡è¨Š', 'è³¼ç¥¨', 'Read More', 'More', '']:
            img = link.find('img')
            if img: title = img.get('alt') or img.get('title')
        
        if not title or title.strip() in ['è©³å…§æ–‡', 'è©³ç´°è³‡è¨Š', 'è³¼ç¥¨', 'Read More', 'More', '']:
            # å˜—è©¦æ‰¾å…§éƒ¨çš„æ¨™é¡Œæ¨™ç±¤ (é‡å°è¯å±±ã€æ¾è¸ç­‰çµæ§‹)
            header_tag = link.find(['h3', 'h4', 'h5', 'div', 'span'], class_=re.compile(r'(title|name|header|subject)', re.I))
            if header_tag: 
                title = header_tag.get_text(strip=True)
            else:
                # æœ€å¾Œæ‰‹æ®µï¼šæŠ“å–æ‰€æœ‰æ–‡å­—
                title = link.get_text(" ", strip=True)

        # æ¸…ç†æ¨™é¡Œ
        if title:
            # ç§»é™¤å¸¸è¦‹ç„¡æ„ç¾©å­—è©
            noise_words = ['ç«‹å³è³¼ç¥¨', 'è©³ç´°å…§å®¹', 'Read More', 'æ´»å‹•è©³æƒ…', 'æŸ¥çœ‹æ›´å¤š', 'å·²çµæŸ', 'å ±å', 'è©³ç´°è³‡è¨Š', 'è³¼ç¥¨', 'More']
            for noise in noise_words:
                title = title.replace(noise, "")
            # ç§»é™¤æ—¥æœŸæ ¼å¼ (ä¾‹å¦‚ 2026/01/01 æˆ– 2026-01-01)
            title = re.sub(r'\d{4}[-/]\d{1,2}[-/]\d{1,2}', '', title)
            title = re.sub(r'\s+', ' ', title).strip()

        # éæ¿¾å¤ªçŸ­æˆ–ä»æ˜¯ç„¡æ•ˆçš„æ¨™é¡Œ
        if not title or len(title) < 3:
            continue

        img_url = None
        img_tag = link.find('img')
        if img_tag: img_url = img_tag.get('src')
        if img_url and not img_url.startswith('http'):
            img_url = urljoin(base_url, img_url)

        events.append({
            'title': title,
            'url': full_url,
            'platform': platform_name,
            'img_url': img_url,
            'date': "è©³å…§æ–‡",
            'type': get_event_category_from_title(title),
            'scraped_at': datetime.now().isoformat()
        })
        seen_urls.add(full_url)

    logger.info(f"[{platform_name}] æœ‰æ•ˆæ´»å‹•æ•¸: {len(events)}")
    return events

# =========================
# ğŸ•·ï¸ å„å¹³å°çˆ¬èŸ²å‡½å¼ (19 å¹³å° - V34 å¾©åˆ»)
# =========================

async def fetch_kktix_events_list(session):
    return await generic_fetch(session, "KKTIX", "https://kktix.com/events", 
                               [f"https://kktix.com/events?category_id={i}" for i in [2,6,4,3,8]] + ["https://kktix.com/"],
                               'a[href*="/events/"], .event-item a, .event-card a')

async def fetch_accupass_events_list(session):
    # ACCUPASS ç‰¹æ®Šè™•ç†
    logger.info("ğŸš€ å•Ÿå‹• ACCUPASS...")
    base_url = "https://www.accupass.com/search"
    target_urls = [f"{base_url}?q={k}" for k in ["éŸ³æ¨‚", "è—æ–‡", "å­¸ç¿’", "ç§‘æŠ€", "å±•è¦½"]] + ["https://www.accupass.com/?area=north"]
    all_events = []
    seen = set()
    for url in target_urls:
        await asyncio.sleep(2)
        html = await fetch_text(session, url, headers=get_headers('https://www.accupass.com/'))
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        candidates = soup.find_all('a', href=re.compile(r'^/event/([A-Za-z0-9]+)'))
        
        logger.info(f"[ACCUPASS] URL {url} æ‰¾åˆ° {len(candidates)} å€‹å€™é¸é€£çµ")
        
        for link in candidates:
            href = link.get('href')
            full_url = urljoin("https://www.accupass.com", href).split('?')[0]
            if full_url in seen: continue
            
            # æ¨™é¡Œå¼·åŒ–
            title = safe_get_text(link.find('h3'))
            if not title: title = safe_get_text(link.find(class_=re.compile(r'title', re.I)))
            if not title: title = safe_get_text(link)
            
            if len(title) < 2: continue
            
            img_tag = link.find('img')
            img_url = img_tag.get('src') if img_tag else None
            
            all_events.append({
                "title": title, "url": full_url, "platform": "ACCUPASS", "date": "è©³å…§æ–‡", "img_url": img_url,
                "type": get_event_category_from_title(title), "scraped_at": datetime.now().isoformat()
            })
            seen.add(full_url)
    logger.info(f"[ACCUPASS] ç¸½å…±æŠ“å– {len(all_events)} ç­†")
    return all_events

# é€šç”¨æŠ“å–å™¨
async def generic_fetch(session, name, base_url, urls, selector, delay=1):
    logger.info(f"ğŸš€ å•Ÿå‹• {name}...")
    if isinstance(urls, str): urls = [urls]
    all_events = []
    seen = set()
    for url in urls:
        await asyncio.sleep(delay)
        html = await fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select(selector)
        logger.info(f"[{name}] URL {url} é¸æ“‡å™¨ '{selector}' æ‰¾åˆ° {len(links)} å€‹åŸå§‹é€£çµ")
        events = filter_links_for_platform(links, base_url, name)
        for e in events:
            if e['url'] not in seen:
                all_events.append(e)
                seen.add(e['url'])
    return all_events

# å„å¹³å°å®šç¾©
async def fetch_tixcraft(s): return await generic_fetch(s, "æ‹“å…ƒå”®ç¥¨", "https://tixcraft.com", ["https://tixcraft.com/activity", "https://tixcraft.com/activity/list/select_type/all"], 'a[href*="/activity/detail/"]')
async def fetch_kham(s): return await generic_fetch(s, "å¯¬å®", "https://kham.com.tw", [f"https://kham.com.tw/application/UTK01/UTK0101_06.aspx?TYPE=1&CATEGORY={i}" for i in [205,231,116,129]], 'a[href*="UTK0201"]')
async def fetch_opentix(s): return await generic_fetch(s, "OPENTIX", "https://www.opentix.life", "https://www.opentix.life/event", 'a[href*="/event/"]')
async def fetch_udn(s): return await generic_fetch(s, "UDNå”®ç¥¨ç¶²", "https://tickets.udnfunlife.com", ["https://tickets.udnfunlife.com/application/UTK01/UTK0101_03.aspx?Category=77&kdid=cateList","https://tickets.udnfunlife.com/application/UTK01/UTK0101_03.aspx?Category=231&kdid=cateList"], 'a[href*="UTK0201"]')
async def fetch_fami(s): return await generic_fetch(s, "FamiTicket", "https://www.famiticket.com.tw", "https://www.famiticket.com.tw/Home", "a[href*='Content/Home/Activity']")
async def fetch_era(s): return await generic_fetch(s, "å¹´ä»£å”®ç¥¨", "https://ticket.com.tw", "https://ticket.com.tw/application/UTK01/UTK0101_06.aspx?TYPE=1&CATEGORY=77", 'a[href*="UTK0201"]')
async def fetch_tixfun(s): return await generic_fetch(s, "TixFunå”®ç¥¨ç¶²", "https://tixfun.com", "https://tixfun.com/UTK0101_?TYPE=1&CATEGORY=77", 'a[href*="UTK0201"]')
async def fetch_eventgo(s): return await generic_fetch(s, "Event Go", "https://eventgo.bnextmedia.com.tw", "https://eventgo.bnextmedia.com.tw/", 'a[href*="/event/detail"]')
async def fetch_beclass(s): return await generic_fetch(s, "BeClass", "https://www.beclass.com", "https://www.beclass.com/default.php?name=ShowList&op=recent", "a[href*='rid=']")
async def fetch_indievox(s): return await generic_fetch(s, "iNDIEVOX", "https://www.indievox.com", "https://www.indievox.com/activity/list", 'a[href*="/activity/detail"]')
async def fetch_ibon(s): return await generic_fetch(s, "ibon", "https://ticket.ibon.com.tw", "https://ticket.ibon.com.tw/Activity/Index", 'a[href*="ActivityDetail"]')
async def fetch_huashan(s): return await generic_fetch(s, "è¯å±±1914", "https://www.huashan1914.com", "https://www.huashan1914.com/w/huashan1914/exhibition", '.card-body a') 
async def fetch_songshan(s): return await generic_fetch(s, "æ¾å±±æ–‡å‰µ", "https://www.songshanculturalpark.org", "https://www.songshanculturalpark.org/exhibition", '.exhibition-list a')
async def fetch_stroll(s): return await generic_fetch(s, "StrollTimes", "https://strolltimes.com", "https://strolltimes.com/", 'h3.post-title a') 
async def fetch_kidsclub(s): return await generic_fetch(s, "KidsClub", "https://kidsclub.com.tw", "https://kidsclub.com.tw/", 'a[href*="/product/"], a[href*="/courses/"]') 
async def fetch_wtc(s): return await generic_fetch(s, "å°åŒ—ä¸–è²¿", "https://www.twtc.com.tw", "https://www.twtc.com.tw/exhibition_list.aspx", 'a[href*="exhibition_detail"]')
async def fetch_cksmh(s): return await generic_fetch(s, "ä¸­æ­£ç´€å¿µå ‚", "https://www.cksmh.gov.tw", "https://www.cksmh.gov.tw/activitybee_list.aspx?n=105", 'a[href*="activitybee_"]')

# =========================
# ğŸ’¾ å­˜æª” & é€šçŸ¥
# =========================
async def send_line_notify(message):
    if not LINE_TOKEN: return
    async with aiohttp.ClientSession() as session:
        await session.post("https://notify-api.line.me/api/notify", headers={"Authorization": f"Bearer {LINE_TOKEN}"}, data={"message": message})

def load_existing_data():
    if not OUTPUT_FILE.exists(): return []
    try:
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except: return []

async def save_data_and_notify(new_events):
    existing_events = load_existing_data()
    existing_map = {e['url']: e for e in existing_events}
    
    added_events = []
    updated_count = 0
    
    for event in new_events:
        url = event['url']
        if url not in existing_map:
            existing_map[url] = event
            added_events.append(event)
        else:
            existing_map[url].update(event)
            updated_count += 1
    
    final_list = list(existing_map.values())
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_list, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“Š è³‡æ–™åº«æ›´æ–°å®Œç•¢ | ç¸½ç­†æ•¸: {len(final_list)} | ğŸ†• æ–°å¢: {len(added_events)} | ğŸ”„ æ›´æ–°: {updated_count}")

    if added_events and LINE_TOKEN:
        logger.info(f"ğŸ“¨ æº–å‚™ç™¼é€ LINE é€šçŸ¥ ({len(added_events)} ç­†)...")
        msg = f"\nğŸ”¥ ç™¼ç¾ {len(added_events)} å€‹æ–°æ´»å‹•ï¼\n"
        for e in added_events[:5]:
            msg += f"\nğŸ“Œ {e['title'][:30]}\nğŸ”— {e['url']}\n"
        if len(added_events) > 5:
            msg += f"\n...é‚„æœ‰ {len(added_events)-5} ç­†ï¼Œè«‹ä¸Šç¶²é æŸ¥çœ‹ï¼"
        await send_line_notify(msg)

async def main():
    logger.info("ğŸ”¥ çˆ¬èŸ²ç¨‹å¼é–‹å§‹åŸ·è¡Œ (V35 Fix)...")
    connector = aiohttp.TCPConnector(ssl=False)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [
            fetch_kktix_events_list(session), fetch_accupass_events_list(session), fetch_tixcraft(session),
            fetch_kham(session), fetch_opentix(session), fetch_udn(session), fetch_fami(session),
            fetch_era(session), fetch_tixfun(session), fetch_eventgo(session), fetch_beclass(session),
            fetch_indievox(session), fetch_ibon(session), fetch_huashan(session), fetch_songshan(session),
            fetch_stroll(session), fetch_kidsclub(session), fetch_wtc(session), fetch_cksmh(session)
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        all_new_events = []
        for res in results:
            if isinstance(res, list): all_new_events.extend(res)
            else: logger.error(f"âŒ ä»»å‹™å¤±æ•—: {res}")

        logger.info(f"ğŸ” æœ¬è¼ªçˆ¬å–åŒ¯ç¸½: å…±æŠ“å–åˆ° {len(all_new_events)} ç­†æœ‰æ•ˆè³‡æ–™")
        await save_data_and_notify(all_new_events)

if __name__ == "__main__":
    asyncio.run(main())
