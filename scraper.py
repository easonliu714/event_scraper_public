# -*- coding: utf-8 -*-
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import random
import json
import re
import time
import logging
import os
from datetime import datetime, timezone, timedelta
from urllib.parse import urljoin
from pathlib import Path
from bs4 import BeautifulSoup

# =========================
# ğŸ› ï¸ è¨­å®šå€
# =========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler("scraper.log", encoding='utf-8', mode='w'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

OUTPUT_DIR = Path("docs")
OUTPUT_FILE = OUTPUT_DIR / "data.json"
LINE_TOKEN = os.environ.get("LINE_TOKEN")

# [V50] å‡ç´šç‰ˆ User-Agent èˆ‡ Headers
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
]

# =========================
# ğŸ§© æ ¸å¿ƒå¼•æ“ï¼šRequests Session
# =========================
def create_session():
    session = requests.Session()
    retries = Retry(total=3, backoff_factor=1, status_forcelist=[403, 429, 500, 502, 503, 504])
    session.mount("https://", HTTPAdapter(max_retries=retries))
    session.mount("http://", HTTPAdapter(max_retries=retries))
    
    session.headers.update({
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
    })
    return session

def fetch_text(session, url, referer=None):
    try:
        if referer:
            session.headers.update({'Referer': referer})
        
        resp = session.get(url, timeout=30, verify=False) # verify=False è§£æ±ºéƒ¨åˆ†èˆŠç¶²ç«™è­‰æ›¸å•é¡Œ
        resp.raise_for_status()
        
        if 'charset' not in resp.headers.get('content-type', '').lower():
            resp.encoding = resp.apparent_encoding
            
        return resp.text
    except Exception as e:
        logger.error(f"ğŸ’¥ è«‹æ±‚å¤±æ•—: {url} - {e}")
        return None

# =========================
# ğŸ§  ç¶²å€ä¿®å¾©åŠ©æ‰‹ (V50 New)
# =========================
def fix_utk_url(domain, raw_url):
    """
    å¼·åˆ¶é‡çµ„ UTK ç³»åˆ— (å¯¬å®/å¹´ä»£/UDN) çš„ç¶²å€ï¼Œç¢ºä¿æœ‰æ•ˆæ€§ã€‚
    åªè¦æœ‰ PRODUCT_ID å°±é‡å»ºæˆæ¨™æº–æ ¼å¼ã€‚
    """
    match = re.search(r'PRODUCT_ID=([A-Za-z0-9]+)', raw_url, re.I)
    if match:
        pid = match.group(1)
        # é‡å°ä¸åŒå¹³å°å¾®èª¿è·¯å¾‘
        if "tixfun" in domain:
            return f"https://{domain}/UTK0201_?PRODUCT_ID={pid}"
        else:
            return f"https://{domain}/application/UTK02/UTK0201_.aspx?PRODUCT_ID={pid}"
    
    # å¦‚æœæŠ“ä¸åˆ° IDï¼Œå›å‚³åŸç¶²å€ (é€šå¸¸æœƒè¢«å¾ŒçºŒéæ¿¾æ‰)
    return raw_url

# =========================
# ğŸ§  æ¨™é¡Œèˆ‡ç‰©ä»¶è™•ç†
# =========================
def extract_smart_title(link_tag):
    title = link_tag.get('title')
    if not title:
        header = link_tag.find(['h3', 'h4', 'h5', 'h6', 'span', 'div'], class_=re.compile(r'(title|name|subject|header|caption)', re.I))
        if header: title = header.get_text(strip=True)
    if not title:
        img = link_tag.find('img')
        if img: title = img.get('alt') or img.get('title')
    if not title:
        text = link_tag.get_text(" ", strip=True)
        if text: title = text
    return title

def create_event_obj(title, url, platform, img_url=None, type_override=None):
    if not title: return None

    # [V50] æ“´å……é»‘åå–®
    noise_keywords = [
        'ç«‹å³è³¼ç¥¨', 'è©³ç´°å…§å®¹', 'Read More', 'æ´»å‹•è©³æƒ…', 'æŸ¥çœ‹æ›´å¤š', 'å·²çµæŸ', 'å ±å', 'è©³ç´°è³‡è¨Š', 'è³¼ç¥¨', 
        'More', 'None', 'æ´»å‹•ä»‹ç´¹', 'Traffic', 'æ›´å¤šè©³æƒ…', 'å…¶ä»–æ´»å‹•', 'é–‹æ”¾æ™‚é–“', 'äº¤é€šè³‡è¨Š', 
        'ç•¶å‰é é¢', 'Current Page', 'Go to page', 'çœ‹æ›´å¤š', 'æŸ¥çœ‹å…¨éƒ¨', 'FamiTicketå…¨ç¶²è³¼ç¥¨ç¶²', 'é¦–é ',
        'æ‰¾æ´»å‹•', 'ä¸‹ä¸€é ', 'å»£å‘Šç‰ˆä½å‡ºç§Ÿ', 'éš±ç§æ¬Šæ”¿ç­–', 'è¼ƒèˆŠçš„æ–‡ç« ', 'è©³ç´°ä»‹ç´¹', 'å›é¦–é ', 'ç¶²ç«™å°è¦½',
        'å…©å´é–€å»³', 'ä¸­å¤®é€šå»Š', 'æœå‹™å°', 'å ‚æ™¯ä»‹ç´¹'
    ]
    
    if title.strip() in noise_keywords: return None

    # æ¸…æ´—
    title = re.sub(r'^(event-)?banner-', '', title, flags=re.I)
    for n in noise_keywords: title = title.replace(n, "")
    title = re.sub(r'\d{4}[-/]\d{1,2}[-/]\d{1,2}', '', title)
    title = re.sub(r'^[Â»\s]+|[Â»\s]+$', '', title).strip()

    if re.match(r'^\d+$', title) or len(title) < 3: return None

    # æ™‚å€ UTC+8
    tw_tz = timezone(timedelta(hours=8))
    scraped_time = datetime.now(tw_tz).isoformat()

    event_type = type_override if type_override else get_event_category_from_title(title)

    return {
        'title': title,
        'url': url,
        'platform': platform,
        'img_url': img_url,
        'date': "è©³å…§æ–‡",
        'type': event_type,
        'scraped_at': scraped_time
    }

def get_event_category_from_title(title):
    if not title: return "å…¶ä»–"
    title_lower = title.lower()
    category_mapping = {
        "éŸ³æ¨‚æœƒ/æ¼”å”±æœƒ": ["éŸ³æ¨‚æœƒ", "æ¼”å”±æœƒ", "ç¨å¥æœƒ", "åˆå”±", "äº¤éŸ¿", "ç®¡æ¨‚", "åœ‹æ¨‚", "å¼¦æ¨‚", "é‹¼ç´", "æç´", "å·¡æ¼”", "fan concert", "fancon", "éŸ³æ¨‚ç¯€", "çˆµå£«", "æ¼”å¥", "æ­Œæ‰‹", "æ¨‚åœ˜", "tour", "live", "concert", "solo", "recital", "é›»éŸ³æ´¾å°", "è—äººè¦‹é¢æœƒ", "éŸ³æ¨‚ç¥­", "Voice", "è²å„ª"],
        "éŸ³æ¨‚åŠ‡/æ­ŒåŠ‡": ["éŸ³æ¨‚åŠ‡", "æ­ŒåŠ‡", "musical", "opera"],
        "æˆ²åŠ‡è¡¨æ¼”": ["æˆ²åŠ‡", "èˆå°åŠ‡", "åŠ‡åœ˜", "åŠ‡å ´", "å–œåŠ‡", "å…¬æ¼”", "æŒä¸­æˆ²", "æ­Œä»”æˆ²", "è±«åŠ‡", "è©±åŠ‡", "ç›¸è²", "å¸ƒè¢‹æˆ²", "äº¬åŠ‡", "å´‘åŠ‡", "è—æ–‡æ´»å‹•"],
        "èˆè¹ˆè¡¨æ¼”": ["èˆè¹ˆ", "èˆä½œ", "èˆåœ˜", "èŠ­è•¾", "èˆåŠ‡", "ç¾ä»£èˆ", "æ°‘æ—èˆ", "è¸¢è¸èˆ", "zumba"],
        "å±•è¦½/åšè¦½": ["å±•è¦½", "ç‰¹å±•", "åšç‰©é¤¨", "ç¾è¡“é¤¨", "è—è¡“å±•", "ç•«å±•", "æ”å½±å±•", "æ–‡ç‰©å±•", "ç§‘å­¸å±•", "åšè¦½æœƒ", "å‹•æ¼«", "å±•å‡º", "è¯å±•", "å€‹å±•"],
        "è¦ªå­æ´»å‹•": ["è¦ªå­", "å…’ç«¥", "å¯¶å¯¶", "å®¶åº­", "å°æœ‹å‹", "ç«¥è©±", "å¡é€š", "å‹•ç•«", "é«”é©—", "ç‡ŸéšŠ", "å†¬ä»¤ç‡Ÿ", "å¤ä»¤ç‡Ÿ"],
        "é›»å½±æ”¾æ˜ ": ["é›»å½±", "å½±å±•", "æ•¸ä½ä¿®å¾©", "æ”¾æ˜ ", "é¦–æ˜ ", "ç´€éŒ„ç‰‡", "å‹•ç•«é›»å½±"],
        "é«”è‚²è³½äº‹": ["æ£’çƒ", "ç±ƒçƒ", "éŒ¦æ¨™è³½", "é‹å‹•æœƒ", "è¶³çƒ", "ç¾½çƒ", "ç¶²çƒ", "é¦¬æ‹‰æ¾", "è·¯è·‘", "æ¸¸æ³³", "é«”æ“", "championship", "éŠæˆ²ç«¶è³½"],
        "è¬›åº§/å·¥ä½œåŠ": ["å·¥ä½œåŠ", "èª²ç¨‹", "å°è®€", "æ²™é¾", "è¬›åº§", "é«”é©—", "ç ”ç¿’", "åŸ¹è¨“", "è«–å£‡", "ç ”è¨æœƒ", "åº§è«‡", "workshop", "è·å ´å·¥ä½œè¡“", "è³‡è¨Šç§‘æŠ€", "AI", "Python", "ç«¶è³½", "å‰µä½œ", "çºç¹"],
        "å¨›æ¨‚è¡¨æ¼”": ["è„«å£ç§€", "é­”è¡“", "é›œæŠ€", "é¦¬æˆ²", "ç‰¹æŠ€", "é­”å¹»", "ç¶œè—", "å¨›æ¨‚", "ç§€å ´", "è¡¨æ¼”ç§€", "ç¤¾ç¾¤æ´»å‹•", "æ´¾å°", "å¸‚é›†"],
        "å…¶ä»–": ["æ—…éŠ", "ç¾é£Ÿ", "å…¬ç›Š"]
    }
    for category, keywords in category_mapping.items():
        if any(keyword in title_lower for keyword in keywords): return category
    return "å…¶ä»–"

# =========================
# ğŸ•·ï¸ å¹³å°çˆ¬èŸ² (V50)
# =========================

def fetch_kktix(session):
    logger.info("ğŸš€ å•Ÿå‹• KKTIX...")
    urls = [f"https://kktix.com/events?category_id={i}" for i in [2,6,4,3,8]] + ["https://kktix.com/"]
    events = []
    seen = set()
    for url in urls:
        html = fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="/events/"], .event-item a, .event-card a')
        for link in links:
            href = link.get('href')
            if not href: continue
            full_url = urljoin("https://kktix.com", href).split('?')[0]
            if full_url in seen: continue
            title = extract_smart_title(link)
            img = link.find('img')
            ev = create_event_obj(title, full_url, "KKTIX", img.get('src') if img else None)
            if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[KKTIX] æŠ“å– {len(events)} ç­†")
    return events

def fetch_accupass(session):
    logger.info("ğŸš€ å•Ÿå‹• ACCUPASS...")
    urls = [f"https://www.accupass.com/search?q={k}" for k in ["éŸ³æ¨‚", "è—æ–‡", "å­¸ç¿’", "ç§‘æŠ€", "å±•è¦½"]] + ["https://www.accupass.com/?area=north"]
    events = []
    seen = set()
    for url in urls:
        html = fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        candidates = soup.find_all('a', href=re.compile(r'^/event/([A-Za-z0-9]+)'))
        for link in candidates:
            href = link.get('href')
            full_url = urljoin("https://www.accupass.com", href).split('?')[0]
            if full_url in seen: continue
            title = extract_smart_title(link)
            img = link.find('img')
            ev = create_event_obj(title, full_url, "ACCUPASS", img.get('src') if img else None)
            if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[ACCUPASS] æŠ“å– {len(events)} ç­†")
    return events

def fetch_tixcraft(session):
    logger.info("ğŸš€ å•Ÿå‹• æ‹“å…ƒ...")
    urls = ["https://tixcraft.com/activity", "https://tixcraft.com/activity/list/select_type/all"]
    events = []
    seen = set()
    for url in urls:
        html = fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="/activity/detail/"]')
        for link in links:
            full_url = urljoin("https://tixcraft.com", link.get('href'))
            if full_url in seen: continue
            title = extract_smart_title(link)
            ev = create_event_obj(title, full_url, "æ‹“å…ƒå”®ç¥¨", None)
            if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[æ‹“å…ƒ] æŠ“å– {len(events)} ç­†")
    return events

def fetch_kham(session):
    logger.info("ğŸš€ å•Ÿå‹• å¯¬å® (V50 Fix)...")
    urls = [f"https://kham.com.tw/application/UTK01/UTK0101_06.aspx?TYPE=1&CATEGORY={i}" for i in [205,231,116,129]]
    events = []
    seen = set()
    for url in urls:
        html = fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="UTK0201_"]') 
        for link in links:
            raw_url = urljoin("https://kham.com.tw", link.get('href'))
            # [V50] é€£çµé‡çµ„
            full_url = fix_utk_url("kham.com.tw", raw_url)
            
            if full_url in seen: continue
            if "PRODUCT_ID" not in full_url: continue # å†æ¬¡ç¢ºèª
            
            title = extract_smart_title(link)
            ev = create_event_obj(title, full_url, "å¯¬å®", None)
            if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[å¯¬å®] æŠ“å– {len(events)} ç­†")
    return events

def fetch_opentix(session):
    logger.info("ğŸš€ å•Ÿå‹• OPENTIX...")
    html = fetch_text(session, "https://www.opentix.life/event")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="/event/"]')
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://www.opentix.life", link.get('href'))
        if full_url in seen: continue
        title = extract_smart_title(link)
        img = link.find('img')
        ev = create_event_obj(title, full_url, "OPENTIX", img.get('src') if img else None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[OPENTIX] æŠ“å– {len(events)} ç­†")
    return events

def fetch_udn(session):
    logger.info("ğŸš€ å•Ÿå‹• UDN (V50 Fix)...")
    categories = [231, 205, 77, 116, 100, 129, 218, 163, 101]
    urls = [f"https://tickets.udnfunlife.com/application/UTK01/UTK0101_03.aspx?Category={c}&kdid=cateList" for c in categories]
    events = []
    seen = set()
    for url in urls:
        html = fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="UTK0201_"]')
        for link in links:
            raw_url = urljoin("https://tickets.udnfunlife.com", link.get('href'))
            # [V50] é€£çµé‡çµ„
            full_url = fix_utk_url("tickets.udnfunlife.com", raw_url)
            
            if full_url in seen: continue
            if "PRODUCT_ID" not in full_url: continue

            title_raw = extract_smart_title(link)
            title = title_raw.split("NT$")[0].strip() if title_raw else ""
            ev = create_event_obj(title, full_url, "UDNå”®ç¥¨ç¶²", None)
            if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[UDN] æŠ“å– {len(events)} ç­†")
    return events

def fetch_fami(session):
    logger.info("ğŸš€ å•Ÿå‹• FamiTicket...")
    html = fetch_text(session, "https://www.famiticket.com.tw/Home/Activity/Search/242")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.find_all('a', href=re.compile(r'Activity', re.I))
    events = []
    seen = set()
    for link in links:
        href = link.get('href')
        full_url = urljoin("https://www.famiticket.com.tw", link.get('href'))
        if full_url in seen: continue
        if "Info" not in full_url and "Search" not in full_url: continue
        title = extract_smart_title(link)
        ev = create_event_obj(title, full_url, "FamiTicket", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[FamiTicket] æŠ“å– {len(events)} ç­†")
    return events

def fetch_era(session):
    logger.info("ğŸš€ å•Ÿå‹• å¹´ä»£ (V50 Fix)...")
    html = fetch_text(session, "https://ticket.com.tw/application/UTK01/UTK0101_06.aspx?TYPE=1&CATEGORY=77")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="UTK0201_"]')
    events = []
    seen = set()
    for link in links:
        raw_url = urljoin("https://ticket.com.tw", link.get('href'))
        # [V50] é€£çµé‡çµ„
        full_url = fix_utk_url("ticket.com.tw", raw_url)
        
        if full_url in seen: continue
        if "PRODUCT_ID" not in full_url: continue

        title = extract_smart_title(link)
        ev = create_event_obj(title, full_url, "å¹´ä»£å”®ç¥¨", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[å¹´ä»£] æŠ“å– {len(events)} ç­†")
    return events

def fetch_tixfun(session):
    logger.info("ğŸš€ å•Ÿå‹• TixFun (V50 Fix)...")
    html = fetch_text(session, "https://tixfun.com/UTK0101_?TYPE=1&CATEGORY=77")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="UTK0201_"]')
    events = []
    seen = set()
    for link in links:
        raw_url = urljoin("https://tixfun.com", link.get('href'))
        # [V50] é€£çµé‡çµ„
        full_url = fix_utk_url("tixfun.com", raw_url)
        
        if full_url in seen: continue
        if "PRODUCT_ID" not in full_url: continue

        title = extract_smart_title(link)
        ev = create_event_obj(title, full_url, "TixFunå”®ç¥¨ç¶²", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[TixFun] æŠ“å– {len(events)} ç­†")
    return events

def fetch_eventgo(session):
    logger.info("ğŸš€ å•Ÿå‹• Event Go...")
    html = fetch_text(session, "https://eventgo.bnextmedia.com.tw/")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="/event/detail"]')
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://eventgo.bnextmedia.com.tw", link.get('href'))
        if full_url in seen: continue
        title = extract_smart_title(link)
        img = link.find('img')
        ev = create_event_obj(title, full_url, "Event Go", img.get('src') if img else None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[Event Go] æŠ“å– {len(events)} ç­†")
    return events

def fetch_beclass(session):
    logger.info("ğŸš€ å•Ÿå‹• BeClass...")
    html = fetch_text(session, "https://www.beclass.com/default.php?name=ShowList&op=recent")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select("a[href*='rid=']")
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://www.beclass.com", link.get('href'))
        if full_url in seen: continue
        title = link.get_text(strip=True)
        ev = create_event_obj(title, full_url, "BeClass", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[BeClass] æŠ“å– {len(events)} ç­†")
    return events

def fetch_indievox(session):
    logger.info("ğŸš€ å•Ÿå‹• iNDIEVOX...")
    html = fetch_text(session, "https://www.indievox.com/activity/list")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="/activity/detail"]')
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://www.indievox.com", link.get('href'))
        if full_url in seen: continue
        title = extract_smart_title(link)
        img = link.find('img')
        ev = create_event_obj(title, full_url, "iNDIEVOX", img.get('src') if img else None, type_override="éŸ³æ¨‚æœƒ/æ¼”å”±æœƒ")
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[iNDIEVOX] æŠ“å– {len(events)} ç­†")
    return events

def fetch_ibon(session):
    logger.info("ğŸš€ å•Ÿå‹• ibon...")
    html = fetch_text(session, "https://ticket.ibon.com.tw/Activity/Index")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    all_links = soup.find_all('a', href=True)
    events = []
    seen = set()
    for link in all_links:
        href = link.get('href')
        if "activity" not in href.lower(): continue
        full_url = urljoin("https://ticket.ibon.com.tw", href)
        if full_url in seen: continue
        title = extract_smart_title(link)
        img = link.find('img')
        ev = create_event_obj(title, full_url, "ibon", img.get('src') if img else None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[ibon] æŠ“å– {len(events)} ç­†")
    return events

def fetch_huashan(session):
    logger.info("ğŸš€ å•Ÿå‹• è¯å±±...")
    html = fetch_text(session, "https://www.huashan1914.com/w/huashan1914/exhibition")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select("a[href*='exhibition_']")
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://www.huashan1914.com", link.get('href'))
        if full_url in seen: continue
        title = link.get_text(strip=True) or link.get('title')
        ev = create_event_obj(title, full_url, "è¯å±±1914", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[è¯å±±] æŠ“å– {len(events)} ç­†")
    return events

def fetch_songshan(session):
    logger.info("ğŸš€ å•Ÿå‹• æ¾å±±...")
    html = fetch_text(session, "https://www.songshanculturalpark.org/exhibition")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.find_all('a', href=re.compile(r'/exhibition/'))
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://www.songshanculturalpark.org", link.get('href'))
        if full_url in seen: continue
        title = extract_smart_title(link)
        img = link.find('img')
        ev = create_event_obj(title, full_url, "æ¾å±±æ–‡å‰µ", img.get('src') if img else None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[æ¾å±±] æŠ“å– {len(events)} ç­†")
    return events

def fetch_stroll(session):
    logger.info("ğŸš€ å•Ÿå‹• StrollTimes (V50 Fix)...")
    # [V50] åŠ å…¥ Referer å½è£
    html = fetch_text(session, "https://strolltimes.com/", referer="https://www.google.com/")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('h3.post-title a')
    events = []
    seen = set()
    for link in links:
        href = link.get('href')
        if not href: continue
        full_url = href
        if full_url in seen: continue
        title = extract_smart_title(link)
        ev = create_event_obj(title, full_url, "StrollTimes", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[StrollTimes] æŠ“å– {len(events)} ç­†")
    return events

def fetch_kidsclub(session):
    logger.info("ğŸš€ å•Ÿå‹• KidsClub...")
    html = fetch_text(session, "https://www.kidsclub.com.tw/")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    all_links = soup.find_all('a', href=True)
    events = []
    seen = set()
    for link in all_links:
        href = link.get('href')
        if "product-category" in href: continue
        if not re.search(r'(product|courses)', href): continue
        full_url = urljoin("https://www.kidsclub.com.tw", href)
        if full_url in seen: continue
        title = extract_smart_title(link)
        img = link.find('img')
        ev = create_event_obj(title, full_url, "KidsClub", img.get('src') if img else None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[KidsClub] æŠ“å– {len(events)} ç­†")
    return events

def fetch_wtc(session):
    logger.info("ğŸš€ å•Ÿå‹• å°åŒ—ä¸–è²¿...")
    url = "https://www.twtc.com.tw/exhibition?p=home"
    html = fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    base_url = "https://www.twtc.com.tw/"
    events = []
    seen = set()
    rows = soup.select("tr")
    for row in rows:
        link = row.select_one("a[href*='detail'], a[href*='id=']")
        if not link: continue
        href = link['href']
        raw_title = link.get_text(strip=True)
        if not raw_title or raw_title.lower() in ['more', 'top', 'è©³ç´°å…§å®¹']:
            candidates = []
            for td in row.find_all('td'):
                txt = td.get_text(strip=True)
                if len(txt) > 4 and not re.match(r'^[\d/\-\.:\s]+$', txt):
                    candidates.append(txt)
            if candidates:
                raw_title = max(candidates, key=len)
        if not raw_title: continue
        full_url = urljoin(base_url, href)
        if full_url in seen: continue
        ev = create_event_obj(raw_title, full_url, "å°åŒ—ä¸–è²¿", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[å°åŒ—ä¸–è²¿] æŠ“å– {len(events)} ç­†")
    return events

def fetch_cksmh(session):
    logger.info("ğŸš€ å•Ÿå‹• ä¸­æ­£ç´€å¿µå ‚ (V50 Fix)...")
    # [V50] ä½¿ç”¨ requests ä¸”æœ‰ headersï¼Œæ‡‰è©²èƒ½æ­£å¸¸æŠ“å–
    html = fetch_text(session, "https://www.cksmh.gov.tw/activitybee_list.aspx?n=105")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="activitybee_"]')
    events = []
    seen = set()
    for link in links:
        href = link.get('href')
        if not href: continue
        full_url = urljoin("https://www.cksmh.gov.tw", href)
        if full_url in seen: continue
        title = extract_smart_title(link)
        ev = create_event_obj(title, full_url, "ä¸­æ­£ç´€å¿µå ‚", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[ä¸­æ­£ç´€å¿µå ‚] æŠ“å– {len(events)} ç­†")
    return events

# =========================
# ğŸ’¾ å­˜æª” & é€šçŸ¥
# =========================
def send_line_notify(message):
    if not LINE_TOKEN: return
    try:
        requests.post(
            "https://notify-api.line.me/api/notify",
            headers={"Authorization": f"Bearer {LINE_TOKEN}"},
            data={"message": message},
            timeout=10
        )
    except Exception as e:
        logger.error(f"Line Notify Error: {e}")

def load_existing_data():
    if not OUTPUT_FILE.exists(): return []
    try:
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except: return []

def save_data_and_notify(new_events):
    existing_events = load_existing_data()
    existing_map = {e['url']: e for e in existing_events}
    added_events = []
    updated_count = 0
    for event in new_events:
        url = event['url']
        if url not in existing_map:
            existing_map[url] = event
            added_events.append(event)
        else:
            existing_map[url].update(event)
            updated_count += 1
    
    final_list = list(existing_map.values())
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_list, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“Š è³‡æ–™åº«æ›´æ–°å®Œç•¢ | ç¸½ç­†æ•¸: {len(final_list)} | ğŸ†• æ–°å¢: {len(added_events)} | ğŸ”„ æ›´æ–°: {updated_count}")

    if added_events and LINE_TOKEN:
        logger.info(f"ğŸ“¨ æº–å‚™ç™¼é€ LINE é€šçŸ¥ ({len(added_events)} ç­†)...")
        msg = f"\nğŸ”¥ ç™¼ç¾ {len(added_events)} å€‹æ–°æ´»å‹•ï¼\n"
        for e in added_events[:5]:
            msg += f"\nğŸ“Œ {e['title'][:30]}\nğŸ”— {e['url']}\n"
        if len(added_events) > 5:
            msg += f"\n...é‚„æœ‰ {len(added_events)-5} ç­†ï¼Œè«‹ä¸Šç¶²é æŸ¥çœ‹ï¼"
        send_line_notify(msg)

def main():
    logger.info("ğŸ”¥ çˆ¬èŸ²ç¨‹å¼é–‹å§‹åŸ·è¡Œ (V50 Golden Version)...")
    session = create_session()
    all_new_events = []
    try:
        all_new_events.extend(fetch_kktix(session))
        all_new_events.extend(fetch_accupass(session))
        all_new_events.extend(fetch_tixcraft(session))
        all_new_events.extend(fetch_kham(session))
        all_new_events.extend(fetch_opentix(session))
        all_new_events.extend(fetch_udn(session))
        all_new_events.extend(fetch_fami(session))
        all_new_events.extend(fetch_era(session))
        all_new_events.extend(fetch_tixfun(session))
        all_new_events.extend(fetch_eventgo(session))
        all_new_events.extend(fetch_beclass(session))
        all_new_events.extend(fetch_indievox(session))
        all_new_events.extend(fetch_ibon(session))
        all_new_events.extend(fetch_huashan(session))
        all_new_events.extend(fetch_songshan(session))
        all_new_events.extend(fetch_stroll(session))
        all_new_events.extend(fetch_kidsclub(session))
        all_new_events.extend(fetch_wtc(session))
        all_new_events.extend(fetch_cksmh(session))
    except Exception as e:
        logger.error(f"âŒ ä¸»ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤: {e}")
    finally:
        session.close()

    logger.info(f"ğŸ” æœ¬è¼ªçˆ¬å–åŒ¯ç¸½: å…±æŠ“å–åˆ° {len(all_new_events)} ç­†æœ‰æ•ˆè³‡æ–™")
    save_data_and_notify(all_new_events)

if __name__ == "__main__":
    main()
