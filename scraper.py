# -*- coding: utf-8 -*-
import asyncio
import aiohttp
import random
import json
import re
import time
import logging
import os
from datetime import datetime, timezone, timedelta
from urllib.parse import urljoin
from pathlib import Path
from bs4 import BeautifulSoup

# =========================
# ğŸ› ï¸ è¨­å®šå€
# =========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler("scraper.log", encoding='utf-8', mode='w'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

OUTPUT_DIR = Path("docs")
OUTPUT_FILE = OUTPUT_DIR / "data.json"
LINE_TOKEN = os.environ.get("LINE_TOKEN")

# æ¨¡æ“¬çœŸå¯¦ Chrome ç€è¦½å™¨
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
]

# =========================
# ğŸ§© è¼”åŠ©å·¥å…·
# =========================
def get_headers(referer=None):
    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0',
    }
    if referer: headers['Referer'] = referer
    return headers

def get_event_category_from_title(title):
    if not title: return "å…¶ä»–"
    title_lower = title.lower()
    category_mapping = {
        "éŸ³æ¨‚æœƒ/æ¼”å”±æœƒ": ["éŸ³æ¨‚æœƒ", "æ¼”å”±æœƒ", "ç¨å¥æœƒ", "åˆå”±", "äº¤éŸ¿", "ç®¡æ¨‚", "åœ‹æ¨‚", "å¼¦æ¨‚", "é‹¼ç´", "æç´", "å·¡æ¼”", "fan concert", "fancon", "éŸ³æ¨‚ç¯€", "çˆµå£«", "æ¼”å¥", "æ­Œæ‰‹", "æ¨‚åœ˜", "tour", "live", "concert", "solo", "recital", "é›»éŸ³æ´¾å°", "è—äººè¦‹é¢æœƒ", "éŸ³æ¨‚ç¥­", "Voice", "è²å„ª"],
        "éŸ³æ¨‚åŠ‡/æ­ŒåŠ‡": ["éŸ³æ¨‚åŠ‡", "æ­ŒåŠ‡", "musical", "opera"],
        "æˆ²åŠ‡è¡¨æ¼”": ["æˆ²åŠ‡", "èˆå°åŠ‡", "åŠ‡åœ˜", "åŠ‡å ´", "å–œåŠ‡", "å…¬æ¼”", "æŒä¸­æˆ²", "æ­Œä»”æˆ²", "è±«åŠ‡", "è©±åŠ‡", "ç›¸è²", "å¸ƒè¢‹æˆ²", "äº¬åŠ‡", "å´‘åŠ‡", "è—æ–‡æ´»å‹•"],
        "èˆè¹ˆè¡¨æ¼”": ["èˆè¹ˆ", "èˆä½œ", "èˆåœ˜", "èŠ­è•¾", "èˆåŠ‡", "ç¾ä»£èˆ", "æ°‘æ—èˆ", "è¸¢è¸èˆ", "zumba"],
        "å±•è¦½/åšè¦½": ["å±•è¦½", "ç‰¹å±•", "åšç‰©é¤¨", "ç¾è¡“é¤¨", "è—è¡“å±•", "ç•«å±•", "æ”å½±å±•", "æ–‡ç‰©å±•", "ç§‘å­¸å±•", "åšè¦½æœƒ", "å‹•æ¼«", "å±•å‡º", "è¯å±•", "å€‹å±•"],
        "è¦ªå­æ´»å‹•": ["è¦ªå­", "å…’ç«¥", "å¯¶å¯¶", "å®¶åº­", "å°æœ‹å‹", "ç«¥è©±", "å¡é€š", "å‹•ç•«", "é«”é©—", "ç‡ŸéšŠ", "å†¬ä»¤ç‡Ÿ", "å¤ä»¤ç‡Ÿ"],
        "é›»å½±æ”¾æ˜ ": ["é›»å½±", "å½±å±•", "æ•¸ä½ä¿®å¾©", "æ”¾æ˜ ", "é¦–æ˜ ", "ç´€éŒ„ç‰‡", "å‹•ç•«é›»å½±"],
        "é«”è‚²è³½äº‹": ["æ£’çƒ", "ç±ƒçƒ", "éŒ¦æ¨™è³½", "é‹å‹•æœƒ", "è¶³çƒ", "ç¾½çƒ", "ç¶²çƒ", "é¦¬æ‹‰æ¾", "è·¯è·‘", "æ¸¸æ³³", "é«”æ“", "championship", "éŠæˆ²ç«¶è³½"],
        "è¬›åº§/å·¥ä½œåŠ": ["å·¥ä½œåŠ", "èª²ç¨‹", "å°è®€", "æ²™é¾", "è¬›åº§", "é«”é©—", "ç ”ç¿’", "åŸ¹è¨“", "è«–å£‡", "ç ”è¨æœƒ", "åº§è«‡", "workshop", "è·å ´å·¥ä½œè¡“", "è³‡è¨Šç§‘æŠ€", "AI", "Python", "ç«¶è³½", "å‰µä½œ", "çºç¹"],
        "å¨›æ¨‚è¡¨æ¼”": ["è„«å£ç§€", "é­”è¡“", "é›œæŠ€", "é¦¬æˆ²", "ç‰¹æŠ€", "é­”å¹»", "ç¶œè—", "å¨›æ¨‚", "ç§€å ´", "è¡¨æ¼”ç§€", "ç¤¾ç¾¤æ´»å‹•", "æ´¾å°", "å¸‚é›†"],
        "å…¶ä»–": ["æ—…éŠ", "ç¾é£Ÿ", "å…¬ç›Š"]
    }
    for category, keywords in category_mapping.items():
        if any(keyword in title_lower for keyword in keywords): return category
    return "å…¶ä»–"

async def fetch_text(session, url, headers=None, timeout_sec=25):
    if not headers: headers = get_headers()
    try:
        async with session.get(url, headers=headers, ssl=False, timeout=timeout_sec) as resp:
            if resp.status != 200:
                logger.warning(f"âŒ HTTP {resp.status} - {url}")
                return None
            content_type = resp.headers.get('Content-Type', '').lower()
            if 'charset' in content_type:
                return await resp.text()
            else:
                data = await resp.read()
                try: return data.decode('utf-8')
                except: 
                    try: return data.decode('big5')
                    except: return data.decode('utf-8', errors='ignore')
    except Exception as e:
        logger.error(f"ğŸ’¥ è«‹æ±‚ç•°å¸¸: {url} - {e}")
        return None

# =========================
# ğŸ§  æ¨™é¡Œæå–èˆ‡ç‰©ä»¶å»ºç«‹
# =========================
def extract_smart_title(link_tag):
    title = link_tag.get('title')
    
    if not title:
        header = link_tag.find(['h3', 'h4', 'h5', 'h6', 'span', 'div'], class_=re.compile(r'(title|name|subject|header|caption)', re.I))
        if header: title = header.get_text(strip=True)

    if not title:
        img = link_tag.find('img')
        if img: title = img.get('alt') or img.get('title')

    if not title:
        text = link_tag.get_text(" ", strip=True)
        if text: title = text

    return title

def create_event_obj(title, url, platform, img_url=None, type_override=None):
    """
    å»ºç«‹æ´»å‹•ç‰©ä»¶ï¼Œä¸¦é€²è¡Œåš´æ ¼çš„è³‡æ–™æ¸…æ´—
    """
    if not title: return None

    # [V48] ç§»é™¤ Accupass çš„ banner å‰ç¶´
    title = re.sub(r'^(event-)?banner-', '', title, flags=re.I)
    
    # [V48] é»‘åå–®éæ¿¾ (æ–°å¢ç”¨æˆ¶æŒ‡å®šè©å½™)
    noise_keywords = [
        'ç«‹å³è³¼ç¥¨', 'è©³ç´°å…§å®¹', 'Read More', 'æ´»å‹•è©³æƒ…', 'æŸ¥çœ‹æ›´å¤š', 'å·²çµæŸ', 'å ±å', 'è©³ç´°è³‡è¨Š', 'è³¼ç¥¨', 
        'More', 'None', 'æ´»å‹•ä»‹ç´¹', 'Traffic', 'æ›´å¤šè©³æƒ…', 'å…¶ä»–æ´»å‹•', 'é–‹æ”¾æ™‚é–“', 'äº¤é€šè³‡è¨Š', 
        'ç•¶å‰é é¢', 'Current Page', 'Go to page', 'çœ‹æ›´å¤š', 'æŸ¥çœ‹å…¨éƒ¨', 'FamiTicketå…¨ç¶²è³¼ç¥¨ç¶²', 'é¦–é ',
        'æ‰¾æ´»å‹•', 'ä¸‹ä¸€é ', 'å»£å‘Šç‰ˆä½å‡ºç§Ÿ', 'éš±ç§æ¬Šæ”¿ç­–', 'è¼ƒèˆŠçš„æ–‡ç« ', 'è©³ç´°ä»‹ç´¹'
    ]
    
    # å¦‚æœæ¨™é¡Œå®Œå…¨ç­‰æ–¼é»‘åå–®è©å½™ï¼Œç›´æ¥ä¸Ÿæ£„
    if title.strip() in noise_keywords: return None

    # éƒ¨åˆ†å–ä»£
    for n in noise_keywords:
        title = title.replace(n, "")
    
    # ç§»é™¤æ—¥æœŸæ ¼å¼ (å¦‚ 2026/01/01)
    title = re.sub(r'\d{4}[-/]\d{1,2}[-/]\d{1,2}', '', title)
    # ç§»é™¤ç´”ç¬¦è™Ÿ (å¦‚ Â» )
    title = re.sub(r'^[Â»\s]+|[Â»\s]+$', '', title).strip()

    # [V48] åš´æ ¼éæ¿¾ï¼š
    # 1. ç´”æ•¸å­— (å¦‚ iNDIEVOX çš„ id)
    # 2. æ¨™é¡ŒéçŸ­
    if re.match(r'^\d+$', title) or len(title) < 2: return None

    # [V48] æ™‚é–“è™•ç†ï¼šå¼·åˆ¶è½‰ç‚ºå°ç£æ™‚é–“ (UTC+8)
    tw_tz = timezone(timedelta(hours=8))
    scraped_time = datetime.now(tw_tz).isoformat()

    # [V48] é¡åˆ¥åˆ¤æ–·
    event_type = type_override if type_override else get_event_category_from_title(title)

    return {
        'title': title,
        'url': url,
        'platform': platform,
        'img_url': img_url,
        'date': "è©³å…§æ–‡",
        'type': event_type,
        'scraped_at': scraped_time
    }

# =========================
# ğŸ•·ï¸ å¹³å°çˆ¬èŸ² (V48 Refined)
# =========================

async def fetch_kktix(session):
    logger.info("ğŸš€ å•Ÿå‹• KKTIX...")
    urls = [f"https://kktix.com/events?category_id={i}" for i in [2,6,4,3,8]] + ["https://kktix.com/"]
    events = []
    seen = set()
    for url in urls:
        await asyncio.sleep(1)
        html = await fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="/events/"], .event-item a, .event-card a')
        for link in links:
            href = link.get('href')
            if not href: continue
            full_url = urljoin("https://kktix.com", href).split('?')[0]
            if full_url in seen: continue
            title = extract_smart_title(link)
            img = link.find('img')
            ev = create_event_obj(title, full_url, "KKTIX", img.get('src') if img else None)
            if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[KKTIX] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_accupass(session):
    logger.info("ğŸš€ å•Ÿå‹• ACCUPASS...")
    urls = [f"https://www.accupass.com/search?q={k}" for k in ["éŸ³æ¨‚", "è—æ–‡", "å­¸ç¿’", "ç§‘æŠ€", "å±•è¦½"]] + ["https://www.accupass.com/?area=north"]
    events = []
    seen = set()
    for url in urls:
        await asyncio.sleep(2)
        html = await fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        candidates = soup.find_all('a', href=re.compile(r'^/event/([A-Za-z0-9]+)'))
        for link in candidates:
            href = link.get('href')
            full_url = urljoin("https://www.accupass.com", href).split('?')[0]
            if full_url in seen: continue
            title = extract_smart_title(link)
            img = link.find('img')
            ev = create_event_obj(title, full_url, "ACCUPASS", img.get('src') if img else None)
            if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[ACCUPASS] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_tixcraft(session):
    logger.info("ğŸš€ å•Ÿå‹• æ‹“å…ƒ...")
    urls = ["https://tixcraft.com/activity", "https://tixcraft.com/activity/list/select_type/all"]
    events = []
    seen = set()
    for url in urls:
        await asyncio.sleep(1)
        html = await fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="/activity/detail/"]')
        for link in links:
            full_url = urljoin("https://tixcraft.com", link.get('href'))
            if full_url in seen: continue
            title = extract_smart_title(link)
            ev = create_event_obj(title, full_url, "æ‹“å…ƒå”®ç¥¨", None)
            if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[æ‹“å…ƒ] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_kham(session):
    logger.info("ğŸš€ å•Ÿå‹• å¯¬å®...")
    urls = [f"https://kham.com.tw/application/UTK01/UTK0101_06.aspx?TYPE=1&CATEGORY={i}" for i in [205,231,116,129]]
    events = []
    seen = set()
    for url in urls:
        await asyncio.sleep(1)
        html = await fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="UTK0201_"]') 
        for link in links:
            full_url = urljoin("https://kham.com.tw", link.get('href'))
            if full_url in seen: continue
            title = extract_smart_title(link)
            ev = create_event_obj(title, full_url, "å¯¬å®", None)
            if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[å¯¬å®] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_opentix(session):
    logger.info("ğŸš€ å•Ÿå‹• OPENTIX...")
    html = await fetch_text(session, "https://www.opentix.life/event")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="/event/"]')
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://www.opentix.life", link.get('href'))
        if full_url in seen: continue
        title = extract_smart_title(link)
        img = link.find('img')
        ev = create_event_obj(title, full_url, "OPENTIX", img.get('src') if img else None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[OPENTIX] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_udn(session):
    logger.info("ğŸš€ å•Ÿå‹• UDN (V48 è£œå®Œ)...")
    # [V48] è£œé½Šæ‰€æœ‰é¡åˆ¥
    categories = [
        231, # å±•è¦½
        205, # æ¼”å”±æœƒ
        77,  # éŸ³æ¨‚
        116, # æˆ²åŠ‡
        100, # èˆè¹ˆ
        129, # è¦ªå­
        218, # é‹å‹•
        163, # è¬›åº§
        101  # é›»å½±
    ]
    urls = [f"https://tickets.udnfunlife.com/application/UTK01/UTK0101_03.aspx?Category={c}&kdid=cateList" for c in categories]
    
    events = []
    seen = set()
    for url in urls:
        await asyncio.sleep(1)
        html = await fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="UTK0201_"]')
        for link in links:
            full_url = urljoin("https://tickets.udnfunlife.com", link.get('href'))
            if full_url in seen: continue
            title_raw = extract_smart_title(link)
            title = title_raw.split("NT$")[0].strip() if title_raw else ""
            ev = create_event_obj(title, full_url, "UDNå”®ç¥¨ç¶²", None)
            if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[UDN] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_fami(session):
    logger.info("ğŸš€ å•Ÿå‹• FamiTicket...")
    html = await fetch_text(session, "https://www.famiticket.com.tw/Home/Activity/Search/242")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.find_all('a', href=re.compile(r'Activity', re.I))
    events = []
    seen = set()
    for link in links:
        href = link.get('href')
        full_url = urljoin("https://www.famiticket.com.tw", link.get('href'))
        if full_url in seen: continue
        if "Info" not in full_url and "Search" not in full_url: continue
        
        title = extract_smart_title(link)
        ev = create_event_obj(title, full_url, "FamiTicket", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[FamiTicket] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_era(session):
    logger.info("ğŸš€ å•Ÿå‹• å¹´ä»£...")
    html = await fetch_text(session, "https://ticket.com.tw/application/UTK01/UTK0101_06.aspx?TYPE=1&CATEGORY=77")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="UTK0201_"]')
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://ticket.com.tw", link.get('href'))
        if full_url in seen: continue
        title = extract_smart_title(link)
        ev = create_event_obj(title, full_url, "å¹´ä»£å”®ç¥¨", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[å¹´ä»£] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_tixfun(session):
    logger.info("ğŸš€ å•Ÿå‹• TixFun...")
    html = await fetch_text(session, "https://tixfun.com/UTK0101_?TYPE=1&CATEGORY=77")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="UTK0201_"]')
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://tixfun.com", link.get('href'))
        if full_url in seen: continue
        title = extract_smart_title(link)
        ev = create_event_obj(title, full_url, "TixFunå”®ç¥¨ç¶²", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[TixFun] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_eventgo(session):
    logger.info("ğŸš€ å•Ÿå‹• Event Go...")
    html = await fetch_text(session, "https://eventgo.bnextmedia.com.tw/")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="/event/detail"]')
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://eventgo.bnextmedia.com.tw", link.get('href'))
        if full_url in seen: continue
        title = extract_smart_title(link)
        img = link.find('img')
        ev = create_event_obj(title, full_url, "Event Go", img.get('src') if img else None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[Event Go] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_beclass(session):
    logger.info("ğŸš€ å•Ÿå‹• BeClass...")
    html = await fetch_text(session, "https://www.beclass.com/default.php?name=ShowList&op=recent")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select("a[href*='rid=']")
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://www.beclass.com", link.get('href'))
        if full_url in seen: continue
        title = safe_get_text(link)
        ev = create_event_obj(title, full_url, "BeClass", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[BeClass] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_indievox(session):
    logger.info("ğŸš€ å•Ÿå‹• iNDIEVOX...")
    html = await fetch_text(session, "https://www.indievox.com/activity/list")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="/activity/detail"]')
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://www.indievox.com", link.get('href'))
        if full_url in seen: continue
        title = extract_smart_title(link)
        img = link.find('img')
        # [V47] å¼·åˆ¶æŒ‡å®šé¡åˆ¥
        ev = create_event_obj(title, full_url, "iNDIEVOX", img.get('src') if img else None, type_override="éŸ³æ¨‚æœƒ/æ¼”å”±æœƒ")
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[iNDIEVOX] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_ibon(session):
    logger.info("ğŸš€ å•Ÿå‹• ibon...")
    html = await fetch_text(session, "https://ticket.ibon.com.tw/Activity/Index")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    all_links = soup.find_all('a', href=True)
    events = []
    seen = set()
    for link in all_links:
        href = link.get('href')
        if "activity" not in href.lower(): continue
        
        full_url = urljoin("https://ticket.ibon.com.tw", href)
        if full_url in seen: continue
        title = extract_smart_title(link)
        img = link.find('img')
        ev = create_event_obj(title, full_url, "ibon", img.get('src') if img else None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[ibon] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_huashan(session):
    logger.info("ğŸš€ å•Ÿå‹• è¯å±±...")
    html = await fetch_text(session, "https://www.huashan1914.com/w/huashan1914/exhibition")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select("a[href*='exhibition_']")
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://www.huashan1914.com", link.get('href'))
        if full_url in seen: continue
        title = link.get_text(strip=True) or link.get('title')
        ev = create_event_obj(title, full_url, "è¯å±±1914", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[è¯å±±] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_songshan(session):
    logger.info("ğŸš€ å•Ÿå‹• æ¾å±±...")
    html = await fetch_text(session, "https://www.songshanculturalpark.org/exhibition")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.find_all('a', href=re.compile(r'/exhibition/'))
    events = []
    seen = set()
    for link in links:
        full_url = urljoin("https://www.songshanculturalpark.org", link.get('href'))
        if full_url in seen: continue
        title = extract_smart_title(link)
        img = link.find('img')
        ev = create_event_obj(title, full_url, "æ¾å±±æ–‡å‰µ", img.get('src') if img else None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[æ¾å±±] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_stroll(session):
    logger.info("ğŸš€ å•Ÿå‹• StrollTimes (V48 ä¿®æ­£)...")
    html = await fetch_text(session, "https://strolltimes.com/")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    
    # [V48] å›æ­¸ show_news çš„ç²¾æº–é¸æ“‡å™¨ï¼Œé¿å…æŠ“åˆ°å´é‚Šæ¬„å»£å‘Š
    links = soup.select('h3.post-title a')
    
    events = []
    seen = set()
    for link in links:
        href = link.get('href')
        if not href: continue
        full_url = href
        if full_url in seen: continue
        title = extract_smart_title(link)
        ev = create_event_obj(title, full_url, "StrollTimes", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[StrollTimes] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_kidsclub(session):
    logger.info("ğŸš€ å•Ÿå‹• KidsClub (V48 ä¿®æ­£)...")
    html = await fetch_text(session, "https://www.kidsclub.com.tw/")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    all_links = soup.find_all('a', href=True)
    events = []
    seen = set()
    for link in all_links:
        href = link.get('href')
        # [V48] æ’é™¤ categoryï¼ŒåªæŠ“å…·é«”ç”¢å“
        if "product-category" in href: continue
        if not re.search(r'(product|courses)', href): continue
        
        full_url = urljoin("https://www.kidsclub.com.tw", href)
        if full_url in seen: continue
        title = extract_smart_title(link)
        img = link.find('img')
        ev = create_event_obj(title, full_url, "KidsClub", img.get('src') if img else None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[KidsClub] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_wtc(session):
    logger.info("ğŸš€ å•Ÿå‹• å°åŒ—ä¸–è²¿...")
    url = "https://www.twtc.com.tw/exhibition?p=home"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    base_url = "https://www.twtc.com.tw/"
    events = []
    seen = set()
    
    rows = soup.select("tr")
    for row in rows:
        link = row.select_one("a[href*='detail'], a[href*='id=']")
        if not link: continue
        
        href = link['href']
        raw_title = link.get_text(strip=True)
        if not raw_title or raw_title.lower() in ['more', 'top', 'è©³ç´°å…§å®¹']:
            candidates = []
            for td in row.find_all('td'):
                txt = td.get_text(strip=True)
                if len(txt) > 4 and not re.match(r'^[\d/\-\.:\s]+$', txt):
                    candidates.append(txt)
            if candidates:
                raw_title = max(candidates, key=len)
        
        if not raw_title: continue
        full_url = urljoin(base_url, href)
        if full_url in seen: continue
        ev = create_event_obj(raw_title, full_url, "å°åŒ—ä¸–è²¿", None)
        if ev: events.append(ev); seen.add(full_url)
        
    logger.info(f"[å°åŒ—ä¸–è²¿] æŠ“å– {len(events)} ç­†")
    return events

async def fetch_cksmh(session):
    logger.info("ğŸš€ å•Ÿå‹• ä¸­æ­£ç´€å¿µå ‚ (V48 ä¿®æ­£)...")
    # [V48] å›æ­¸ show_news ç¶²å€ï¼ŒåªæŠ“æ´»å‹•åˆ—è¡¨
    html = await fetch_text(session, "https://www.cksmh.gov.tw/activitybee_list.aspx?n=105")
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    
    # [V48] ç²¾æº–éæ¿¾
    links = soup.select('a[href*="activitybee_"]')
    
    events = []
    seen = set()
    for link in links:
        href = link.get('href')
        if not href: continue
        full_url = urljoin("https://www.cksmh.gov.tw", href)
        if full_url in seen: continue
        title = extract_smart_title(link)
        ev = create_event_obj(title, full_url, "ä¸­æ­£ç´€å¿µå ‚", None)
        if ev: events.append(ev); seen.add(full_url)
    logger.info(f"[ä¸­æ­£ç´€å¿µå ‚] æŠ“å– {len(events)} ç­†")
    return events

# =========================
# ğŸ’¾ å­˜æª” & é€šçŸ¥
# =========================
async def send_line_notify(message):
    if not LINE_TOKEN: return
    async with aiohttp.ClientSession() as session:
        await session.post("https://notify-api.line.me/api/notify", headers={"Authorization": f"Bearer {LINE_TOKEN}"}, data={"message": message})

def load_existing_data():
    if not OUTPUT_FILE.exists(): return []
    try:
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except: return []

async def save_data_and_notify(new_events):
    existing_events = load_existing_data()
    existing_map = {e['url']: e for e in existing_events}
    
    added_events = []
    updated_count = 0
    
    for event in new_events:
        url = event['url']
        if url not in existing_map:
            existing_map[url] = event
            added_events.append(event)
        else:
            existing_map[url].update(event)
            updated_count += 1
    
    final_list = list(existing_map.values())
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_list, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“Š è³‡æ–™åº«æ›´æ–°å®Œç•¢ | ç¸½ç­†æ•¸: {len(final_list)} | ğŸ†• æ–°å¢: {len(added_events)} | ğŸ”„ æ›´æ–°: {updated_count}")

    if added_events and LINE_TOKEN:
        logger.info(f"ğŸ“¨ æº–å‚™ç™¼é€ LINE é€šçŸ¥ ({len(added_events)} ç­†)...")
        msg = f"\nğŸ”¥ ç™¼ç¾ {len(added_events)} å€‹æ–°æ´»å‹•ï¼\n"
        for e in added_events[:5]:
            msg += f"\nğŸ“Œ {e['title'][:30]}\nğŸ”— {e['url']}\n"
        if len(added_events) > 5:
            msg += f"\n...é‚„æœ‰ {len(added_events)-5} ç­†ï¼Œè«‹ä¸Šç¶²é æŸ¥çœ‹ï¼"
        await send_line_notify(msg)

async def main():
    logger.info("ğŸ”¥ çˆ¬èŸ²ç¨‹å¼é–‹å§‹åŸ·è¡Œ (V48 Final Polish)...")
    connector = aiohttp.TCPConnector(ssl=False)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [
            fetch_kktix(session), fetch_accupass(session), fetch_tixcraft(session),
            fetch_kham(session), fetch_opentix(session), fetch_udn(session), fetch_fami(session),
            fetch_era(session), fetch_tixfun(session), fetch_eventgo(session), fetch_beclass(session),
            fetch_indievox(session), fetch_ibon(session), fetch_huashan(session), fetch_songshan(session),
            fetch_stroll(session), fetch_kidsclub(session), fetch_wtc(session), fetch_cksmh(session)
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        all_new_events = []
        for res in results:
            if isinstance(res, list): all_new_events.extend(res)
            else: logger.error(f"âŒ ä»»å‹™å¤±æ•—: {res}")

        logger.info(f"ğŸ” æœ¬è¼ªçˆ¬å–åŒ¯ç¸½: å…±æŠ“å–åˆ° {len(all_new_events)} ç­†æœ‰æ•ˆè³‡æ–™")
        await save_data_and_notify(all_new_events)

if __name__ == "__main__":
    asyncio.run(main())
