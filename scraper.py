# -*- coding: utf-8 -*-
import asyncio
import aiohttp
import random
import json
import re
import time
import logging
from datetime import datetime
from urllib.parse import urljoin
from pathlib import Path
from bs4 import BeautifulSoup

# =========================
# ğŸ› ï¸ è¨­å®šå€ & æ—¥èªŒé…ç½®
# =========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

OUTPUT_DIR = Path("docs")
OUTPUT_FILE = OUTPUT_DIR / "data.json"

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
]

# è©³ç´°é ç¶²å€ç™½åå–® (æ–°å¢ V34 æ‰€æœ‰å¹³å°)
DETAIL_URL_WHITELIST = {
    "æ‹“å…ƒå”®ç¥¨": re.compile(r"^https?://(www\.)?tixcraft\.com/activity/detail/[A-Za-z0-9_-]+", re.I),
    "KKTIX": re.compile(r"^https?://[a-z0-9-]+\.kktix\.cc/events/[A-Za-z0-9-_]+", re.I),
    "OPENTIX": re.compile(r"^https?://(www\.)?opentix\.life/event/\d+", re.I),
    "å¹´ä»£å”®ç¥¨": re.compile(r"^https?://(www\.)?ticket\.com\.tw/application/UTK02/UTK0201_\.aspx\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "UDNå”®ç¥¨ç¶²": re.compile(r"^https?://(www\.)?tickets\.udnfunlife\.com/application/UTK02/UTK0201_\.aspx\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "TixFunå”®ç¥¨ç¶²": re.compile(r"^https?://(www\.)?tixfun\.com/UTK0201_\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "å¯¬å®": re.compile(r"^https?://(www\.)?kham\.com\.tw/application/UTK02/UTK0201_\.aspx\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "Event Go": re.compile(r"^https?://eventgo\.bnextmedia\.com\.tw/event/detail[^\s]*$", re.I),
    "iNDIEVOX": re.compile(r"^https?://(www\.)?indievox\.com/activity/detail/[0-9_]+", re.I),
    "ibon": re.compile(r"^https?://ticket\.ibon\.com\.tw/ActivityDetail/.*", re.I),
    "è¯å±±1914": re.compile(r"^https?://(www\.)?huashan1914\.com/w/huashan1914/exhibition.*", re.I),
    "æ¾å±±æ–‡å‰µ": re.compile(r"^https?://(www\.)?songshanculturalpark\.org/exhibition.*", re.I),
    "KidsClub": re.compile(r"^https?://(www\.)?kidsclub\.com\.tw/.*", re.I),
    "StrollTimes": re.compile(r"^https?://strolltimes\.com/.*", re.I),
    "å°åŒ—ä¸–è²¿": re.compile(r"^https?://(www\.)?twtc\.com\.tw/.*", re.I),
    "ä¸­æ­£ç´€å¿µå ‚": re.compile(r"^https?://(www\.)?cksmh\.gov\.tw/.*", re.I),
}

# =========================
# ğŸ§© è¼”åŠ©å·¥å…·å‡½å¼
# =========================
def get_headers(referer=None):
    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
    }
    if referer: headers['Referer'] = referer
    return headers

def safe_get_text(element, default="è©³å…§æ–‡"):
    if element and hasattr(element, 'get_text'):
        text = element.get_text(strip=True)
        return text if text else default
    return default

def get_event_category_from_title(title):
    if not title: return "å…¶ä»–"
    title_lower = title.lower()
    category_mapping = {
        "éŸ³æ¨‚æœƒ/æ¼”å”±æœƒ": ["éŸ³æ¨‚æœƒ", "æ¼”å”±æœƒ", "ç¨å¥æœƒ", "åˆå”±", "äº¤éŸ¿", "ç®¡æ¨‚", "åœ‹æ¨‚", "å¼¦æ¨‚", "é‹¼ç´", "æç´", "å·¡æ¼”", "fan concert", "fancon", "éŸ³æ¨‚ç¯€", "çˆµå£«", "æ¼”å¥", "æ­Œæ‰‹", "æ¨‚åœ˜", "tour", "live", "concert", "solo", "recital", "é›»éŸ³æ´¾å°", "è—äººè¦‹é¢æœƒ"],
        "éŸ³æ¨‚åŠ‡/æ­ŒåŠ‡": ["éŸ³æ¨‚åŠ‡", "æ­ŒåŠ‡", "musical", "opera"],
        "æˆ²åŠ‡è¡¨æ¼”": ["æˆ²åŠ‡", "èˆå°åŠ‡", "åŠ‡åœ˜", "åŠ‡å ´", "å–œåŠ‡", "å…¬æ¼”", "æŒä¸­æˆ²", "æ­Œä»”æˆ²", "è±«åŠ‡", "è©±åŠ‡", "ç›¸è²", "å¸ƒè¢‹æˆ²", "äº¬åŠ‡", "å´‘åŠ‡", "è—æ–‡æ´»å‹•"],
        "èˆè¹ˆè¡¨æ¼”": ["èˆè¹ˆ", "èˆä½œ", "èˆåœ˜", "èŠ­è•¾", "èˆåŠ‡", "ç¾ä»£èˆ", "æ°‘æ—èˆ", "è¸¢è¸èˆ", "zumba"],
        "å±•è¦½/åšè¦½": ["å±•è¦½", "ç‰¹å±•", "åšç‰©é¤¨", "ç¾è¡“é¤¨", "è—è¡“å±•", "ç•«å±•", "æ”å½±å±•", "æ–‡ç‰©å±•", "ç§‘å­¸å±•", "åšè¦½æœƒ", "å‹•æ¼«", "å±•å‡º"],
        "è¦ªå­æ´»å‹•": ["è¦ªå­", "å…’ç«¥", "å¯¶å¯¶", "å®¶åº­", "å°æœ‹å‹", "ç«¥è©±", "å¡é€š", "å‹•ç•«", "é«”é©—"],
        "é›»å½±æ”¾æ˜ ": ["é›»å½±", "å½±å±•", "æ•¸ä½ä¿®å¾©", "æ”¾æ˜ ", "é¦–æ˜ ", "ç´€éŒ„ç‰‡", "å‹•ç•«é›»å½±"],
        "é«”è‚²è³½äº‹": ["æ£’çƒ", "ç±ƒçƒ", "éŒ¦æ¨™è³½", "é‹å‹•æœƒ", "è¶³çƒ", "ç¾½çƒ", "ç¶²çƒ", "é¦¬æ‹‰æ¾", "è·¯è·‘", "æ¸¸æ³³", "é«”æ“", "championship", "éŠæˆ²ç«¶è³½"],
        "è¬›åº§/å·¥ä½œåŠ": ["å·¥ä½œåŠ", "èª²ç¨‹", "å°è®€", "æ²™é¾", "è¬›åº§", "é«”é©—", "ç ”ç¿’", "åŸ¹è¨“", "è«–å£‡", "ç ”è¨æœƒ", "åº§è«‡", "workshop", "è·å ´å·¥ä½œè¡“", "è³‡è¨Šç§‘æŠ€", "AI", "Python", "ç«¶è³½", "å‰µä½œ", "çºç¹"],
        "å¨›æ¨‚è¡¨æ¼”": ["è„«å£ç§€", "é­”è¡“", "é›œæŠ€", "é¦¬æˆ²", "ç‰¹æŠ€", "é­”å¹»", "ç¶œè—", "å¨›æ¨‚", "ç§€å ´", "è¡¨æ¼”ç§€", "ç¤¾ç¾¤æ´»å‹•", "æ´¾å°", "å¸‚é›†"],
        "å…¶ä»–": ["æ—…éŠ", "ç¾é£Ÿ", "å…¬ç›Š"]
    }
    for category, keywords in category_mapping.items():
        if any(keyword in title_lower for keyword in keywords):
            return category
    return "å…¶ä»–"

async def fetch_text(session, url, headers=None, timeout_sec=15):
    if not headers: headers = get_headers()
    try:
        start_time = time.time()
        async with session.get(url, headers=headers, ssl=False, timeout=timeout_sec) as resp:
            duration = time.time() - start_time
            if resp.status != 200:
                logger.warning(f"âŒ HTTP {resp.status} - {url}")
                return None
            text = await resp.text()
            return text
    except asyncio.TimeoutError:
        logger.error(f"â³ è«‹æ±‚é€¾æ™‚: {url}")
        return None
    except Exception as e:
        logger.error(f"ğŸ’¥ è«‹æ±‚ç•°å¸¸: {url} - {e}")
        return None

def filter_links_for_platform(links, base_url, platform_name):
    events = []
    seen_urls = set()
    wl = DETAIL_URL_WHITELIST.get(platform_name)

    for link in links:
        href = link.get('href', '')
        if not href: continue
        full_url = urljoin(base_url, href).split('#')[0]

        if platform_name == "Event Go" and not full_url.startswith("https://eventgo.bnextmedia.com.tw/event/detail"): 
            continue
            
        if full_url in seen_urls: continue
        # è‹¥è©²å¹³å°æœ‰ç™½åå–®è¨­å®šï¼Œå‰‡æª¢æŸ¥æ˜¯å¦ç¬¦åˆ
        if wl and not wl.match(full_url): continue

        if platform_name == "UDNå”®ç¥¨ç¶²":
            title_text = safe_get_text(link, '')
            if "NT$" in title_text: 
                title_text = title_text.split("NT$")[0].strip()
            title = title_text
        else:
            title = link.get('title')
            if not title:
                img = link.find('img')
                if img: title = img.get('alt')
            if not title:
                title = safe_get_text(link)

        if title:
            title = title.strip()
            title = re.sub(r'\s+', ' ', title) 

        if not title or len(title) < 3 or title in ['æ›´å¤š', 'è©³æƒ…', 'è³¼ç¥¨', 'ç«‹å³è³¼è²·', 'è©³ç´°è³‡è¨Š', 'Read More']:
            continue

        img_url = None
        img_tag = link.find('img')
        if img_tag: img_url = img_tag.get('src')
        
        # åœ–ç‰‡è·¯å¾‘ä¿®æ­£
        if img_url and not img_url.startswith('http'):
            img_url = urljoin(base_url, img_url)

        events.append({
            'title': title,
            'url': full_url,
            'platform': platform_name,
            'img_url': img_url,
            'date': "è©³å…§æ–‡",
            'type': get_event_category_from_title(title),
            'scraped_at': datetime.now().isoformat()
        })
        seen_urls.add(full_url)

    logger.info(f"[{platform_name}] è§£æå®Œæˆ: æ‰¾åˆ° {len(events)} ç­†")
    return events

# =========================
# ğŸ•·ï¸ å„å¹³å°çˆ¬èŸ²å‡½å¼ (V34 å…¨æ”¶éŒ„)
# =========================

# 1. KKTIX
async def fetch_kktix_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• KKTIX çˆ¬èŸ²...")
    base_url = "https://kktix.com/events"
    categories = [
        f"{base_url}?category_id=2", f"{base_url}?category_id=6",
        f"{base_url}?category_id=4", f"{base_url}?category_id=3",
        f"{base_url}?category_id=8", "https://kktix.com/"
    ]
    all_events = []
    seen = set()
    for url in categories:
        await asyncio.sleep(random.uniform(1.5, 3))
        html = await fetch_text(session, url, headers=get_headers('https://kktix.com/'))
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="/events/"], .event-item a, .event-card a')
        events = filter_links_for_platform(links, "https://kktix.com/", "KKTIX")
        for e in events:
            if e['url'] not in seen:
                all_events.append(e)
                seen.add(e['url'])
    return all_events

# 2. ACCUPASS
async def fetch_accupass_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• ACCUPASS çˆ¬èŸ²...")
    base_url = "https://www.accupass.com/search"
    target_urls = [
        f"{base_url}?q=éŸ³æ¨‚", f"{base_url}?q=è—æ–‡",
        f"{base_url}?q=å­¸ç¿’", f"{base_url}?q=ç§‘æŠ€",
        "https://www.accupass.com/?area=north"
    ]
    all_events = []
    seen = set()

    for url in target_urls:
        await asyncio.sleep(random.uniform(2, 4))
        html = await fetch_text(session, url, headers=get_headers('https://www.accupass.com/'))
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        candidates = soup.find_all('a', href=re.compile(r'^/event/([A-Za-z0-9]+)'))
        for link in candidates:
            href = link.get('href')
            if not href or 'javascript' in href: continue
            full_url = urljoin("https://www.accupass.com", href).split('?')[0]
            if full_url in seen: continue
            
            title = safe_get_text(link.find('h3')) or safe_get_text(link)
            if len(title) < 2: continue
            
            img_tag = link.find('img')
            img_url = img_tag.get('src') if img_tag else None

            all_events.append({
                "title": title, "url": full_url, "platform": "ACCUPASS",
                "date": "è©³å…§æ–‡", "img_url": img_url,
                "type": get_event_category_from_title(title),
                "scraped_at": datetime.now().isoformat()
            })
            seen.add(full_url)
    logger.info(f"[ACCUPASS] ç¸½å…±æŠ“å– {len(all_events)} ç­†")
    return all_events

# 3. æ‹“å…ƒ
async def fetch_tixcraft_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• æ‹“å…ƒå”®ç¥¨ çˆ¬èŸ²...")
    urls = ["https://tixcraft.com/activity", "https://tixcraft.com/activity/list/select_type/all"]
    all_events = []
    seen = set()
    for url in urls:
        await asyncio.sleep(1.5)
        html = await fetch_text(session, url, headers=get_headers('https://tixcraft.com/'))
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="/activity/detail/"]')
        events = filter_links_for_platform(links, "https://tixcraft.com/", "æ‹“å…ƒå”®ç¥¨")
        for e in events:
            if e['url'] not in seen:
                all_events.append(e)
                seen.add(e['url'])
    return all_events

# 4. å¯¬å® (ä¿®æ­£é¸æ“‡å™¨)
async def fetch_kham_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• å¯¬å®å”®ç¥¨ çˆ¬èŸ²...")
    category_map = {
        "https://kham.com.tw/application/UTK01/UTK0101_06.aspx?TYPE=1&CATEGORY=205": "éŸ³æ¨‚æœƒ/æ¼”å”±æœƒ", 
        "https://kham.com.tw/application/UTK01/UTK0101_06.aspx?TYPE=1&CATEGORY=231": "å±•è¦½/åšè¦½",
        "https://kham.com.tw/application/UTK01/UTK0101_06.aspx?TYPE=1&CATEGORY=116": "æˆ²åŠ‡è¡¨æ¼”",
    }
    all_events = []
    seen = set()
    for url, cat in category_map.items():
        await asyncio.sleep(1)
        html = await fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="UTK0201"]') # V34 åŸå§‹é¸æ“‡å™¨
        events = filter_links_for_platform(links, "https://kham.com.tw/", "å¯¬å®")
        for e in events:
            if e['url'] not in seen:
                e['type'] = cat 
                all_events.append(e)
                seen.add(e['url'])
    return all_events

# 5. OPENTIX
async def fetch_opentix_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• OPENTIX çˆ¬èŸ²...")
    url = "https://www.opentix.life/event"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="/event/"]')
    return filter_links_for_platform(links, "https://www.opentix.life/", "OPENTIX")

# 6. UDN
async def fetch_udn_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• UDNå”®ç¥¨ çˆ¬èŸ²...")
    urls = [
        "https://tickets.udnfunlife.com/application/UTK01/UTK0101_03.aspx?Category=77&kdid=cateList", 
        "https://tickets.udnfunlife.com/application/UTK01/UTK0101_03.aspx?Category=231&kdid=cateList"
    ]
    all_events = []
    seen = set()
    for url in urls:
        await asyncio.sleep(1)
        html = await fetch_text(session, url)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select('a[href*="UTK0201"]')
        events = filter_links_for_platform(links, "https://tickets.udnfunlife.com/", "UDNå”®ç¥¨ç¶²")
        for e in events:
            if e['url'] not in seen:
                all_events.append(e)
                seen.add(e['url'])
    return all_events

# 7. FamiTicket
async def fetch_famiticket_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• FamiTicket çˆ¬èŸ²...")
    url = "https://www.famiticket.com.tw/Home"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select("a[href*='Content/Home/Activity']")
    return filter_links_for_platform(links, "https://www.famiticket.com.tw", "FamiTicket")

# 8. å¹´ä»£
async def fetch_era_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• å¹´ä»£å”®ç¥¨ çˆ¬èŸ²...")
    url = "https://ticket.com.tw/application/UTK01/UTK0101_06.aspx?TYPE=1&CATEGORY=77" 
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="UTK0201"]')
    return filter_links_for_platform(links, "https://ticket.com.tw", "å¹´ä»£å”®ç¥¨")

# 9. TixFun
async def fetch_tixfun_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• TixFun çˆ¬èŸ²...")
    url = "https://tixfun.com/UTK0101_?TYPE=1&CATEGORY=77"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="UTK0201"]')
    return filter_links_for_platform(links, "https://tixfun.com", "TixFunå”®ç¥¨ç¶²")

# 10. Event Go
async def fetch_eventgo_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• Event Go çˆ¬èŸ²...")
    url = "https://eventgo.bnextmedia.com.tw/"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="/event/detail"]')
    return filter_links_for_platform(links, url, "Event Go")

# 11. BeClass
async def fetch_beclass_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• BeClass çˆ¬èŸ²...")
    url = "https://www.beclass.com/default.php?name=ShowList&op=recent"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select("a[href*='rid=']")
    return filter_links_for_platform(links, "https://www.beclass.com", "BeClass")

# 12. iNDIEVOX
async def fetch_indievox_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• iNDIEVOX çˆ¬èŸ²...")
    url = "https://www.indievox.com/activity/list"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="/activity/detail"]')
    return filter_links_for_platform(links, "https://www.indievox.com", "iNDIEVOX")

# 13. ibon (V34 é‚è¼¯)
async def fetch_ibon_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• ibon çˆ¬èŸ²...")
    url = "https://ticket.ibon.com.tw/Activity/Index"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="ActivityDetail"]')
    return filter_links_for_platform(links, "https://ticket.ibon.com.tw", "ibon")

# 14. è¯å±±1914 (æ–°å¢)
async def fetch_huashan_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• è¯å±±1914 çˆ¬èŸ²...")
    url = "https://www.huashan1914.com/w/huashan1914/exhibition"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('.card-body a') # ä¾V34ç¿’æ…£
    return filter_links_for_platform(links, "https://www.huashan1914.com", "è¯å±±1914")

# 15. æ¾å±±æ–‡å‰µ (æ–°å¢)
async def fetch_songshan_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• æ¾å±±æ–‡å‰µ çˆ¬èŸ²...")
    url = "https://www.songshanculturalpark.org/exhibition"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('.exhibition-list a')
    return filter_links_for_platform(links, "https://www.songshanculturalpark.org", "æ¾å±±æ–‡å‰µ")

# 16. StrollTimes (æ–°å¢)
async def fetch_stroll_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• StrollTimes çˆ¬èŸ²...")
    url = "https://strolltimes.com/"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href]')
    # StrollTimes ç¶²å€éæ¿¾è¼ƒå¯¬é¬†ï¼Œä¾è³´ç™½åå–®
    return filter_links_for_platform(links, "https://strolltimes.com", "StrollTimes")

# 17. KidsClub (æ–°å¢)
async def fetch_kidsclub_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• KidsClub çˆ¬èŸ²...")
    url = "https://www.kidsclub.com.tw/"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href]')
    return filter_links_for_platform(links, "https://www.kidsclub.com.tw", "KidsClub")

# 18. å°åŒ—ä¸–è²¿ (æ–°å¢)
async def fetch_wtc_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• å°åŒ—ä¸–è²¿ çˆ¬èŸ²...")
    url = "https://www.twtc.com.tw/exhibition_list.aspx"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="exhibition_detail"]')
    return filter_links_for_platform(links, "https://www.twtc.com.tw", "å°åŒ—ä¸–è²¿")

# 19. ä¸­æ­£ç´€å¿µå ‚ (æ–°å¢)
async def fetch_cksmh_events_list(session):
    logger.info("ğŸš€ å•Ÿå‹• ä¸­æ­£ç´€å¿µå ‚ çˆ¬èŸ²...")
    url = "https://www.cksmh.gov.tw/activitybee_list.aspx?n=105"
    html = await fetch_text(session, url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select('a[href*="activitybee_"]')
    return filter_links_for_platform(links, "https://www.cksmh.gov.tw", "ä¸­æ­£ç´€å¿µå ‚")


# =========================
# ğŸ’¾ æ ¸å¿ƒé‚è¼¯ï¼šè³‡æ–™è™•ç†èˆ‡å­˜æª”
# =========================
def load_existing_data():
    if not OUTPUT_FILE.exists():
        logger.info("âš ï¸ æ‰¾ä¸åˆ°ç¾æœ‰è³‡æ–™åº«ï¼Œå°‡å»ºç«‹æ–°æª”æ¡ˆ")
        return []
    try:
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            logger.info(f"ğŸ“‚ æˆåŠŸè®€å–ç¾æœ‰è³‡æ–™åº«: {len(data)} ç­†")
            return data
    except Exception as e:
        logger.error(f"âŒ è®€å–è³‡æ–™åº«å¤±æ•—: {e}ï¼Œå°‡å»ºç«‹æ–°æª”æ¡ˆ")
        return []

def save_data(new_events):
    existing_events = load_existing_data()
    existing_map = {e['url']: e for e in existing_events}
    
    added_count = 0
    updated_count = 0

    for event in new_events:
        url = event['url']
        if url not in existing_map:
            existing_map[url] = event
            added_count += 1
        else:
            existing_map[url].update(event) 
            updated_count += 1
    
    final_list = list(existing_map.values())
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_list, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ’¾ è³‡æ–™åº«æ›´æ–°å®Œæˆ")
    logger.info(f"ğŸ“Š ç¸½ç­†æ•¸: {len(final_list)} | ğŸ†• æ–°å¢: {added_count} | ğŸ”„ æ›´æ–°: {updated_count}")

async def main():
    logger.info("ğŸ”¥ çˆ¬èŸ²ç¨‹å¼é–‹å§‹åŸ·è¡Œ (Web Version V34 Full)...")
    
    connector = aiohttp.TCPConnector(ssl=False)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [
            fetch_kktix_events_list(session),
            fetch_accupass_events_list(session),
            fetch_tixcraft_events_list(session),
            fetch_kham_events_list(session),
            fetch_opentix_events_list(session),
            fetch_udn_events_list(session),
            fetch_famiticket_events_list(session),
            fetch_era_events_list(session),
            fetch_tixfun_events_list(session),
            fetch_eventgo_events_list(session),
            fetch_beclass_events_list(session),
            fetch_indievox_events_list(session),
            fetch_ibon_events_list(session),
            fetch_huashan_events_list(session), # æ–°å¢
            fetch_songshan_events_list(session), # æ–°å¢
            fetch_stroll_events_list(session), # æ–°å¢
            fetch_kidsclub_events_list(session), # æ–°å¢
            fetch_wtc_events_list(session), # æ–°å¢
            fetch_cksmh_events_list(session) # æ–°å¢
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_new_events = []
        for res in results:
            if isinstance(res, list):
                all_new_events.extend(res)
            else:
                logger.error(f"âŒ æŸå¹³å°ä»»å‹™å¤±æ•—: {res}")

        logger.info(f"ğŸ” æœ¬è¼ªçˆ¬å–åŒ¯ç¸½: å…±æŠ“å–åˆ° {len(all_new_events)} ç­†æœ‰æ•ˆè³‡æ–™")
        save_data(all_new_events)

if __name__ == "__main__":
    asyncio.run(main())
